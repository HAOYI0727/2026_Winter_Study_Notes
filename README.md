# 2026_Winter_Study_Notes

## 项目结构
```
2026_WINTER_STUDY_NOTES
├── Code Practice/          # 代码实战
│   ├── 4-3.Transformer/    # Transformer架构从零实现代码--2026.02.01~04
│   ├── 6-1.LLaMA2/         # LLaMA2 架构解析与推理实战代码--2026.02.08
│   ├── 6-2.MoE/            # 混合专家模型（MoE）实战工程--2026.02.09
│   ├── 11.LoRA/            # LoRA 微调核心代码实现--2026.02.13~16
│   ├── 13.Quantify/        # 大模型量化实战代码--2026.02.22
│   └── CNN+ResNet/         # CNN与残差网络从零实现代码--2026.01.27
├── Note Summary/           # 学习笔记
│   ├── 2.文本表示与词向量.md    #2026.01.29
│   ├── 3.循环神经网络.md    #2026.01.31
│   ├── 4.注意力机制与Transformer.md    #2026.02.04
│   ├── 5.预训练模型.md    #2026.02.07
│   ├── 6.深入大模型架构.md    #2026.02.10
│   ├── 11-1.PEFT技术综述.md    #2026.02.11
│   ├── 11-2.LoRA方法详解.md    #2026.02.12
│   ├── 11-3.基于 Peft 库的 LoRA 实战.md    #2026.02.14
│   ├── 11-4.Qwen2.5微调实战.md    #2026.02.15
│   ├── 12-1.RLHF 技术详解.md    #2026.02.20
│   ├── 12-2.LLaMA-Factory RLHF（DPO）实战.md    #2026.02.21
│   ├── 13-1.模型量化实战.md    #2026.02.22
│   └── 13-2.DeepSpeed框架介绍.md    #2026.02.23
└── README.md
```

### 1. 深度学习基础模块
- 计算机视觉核心：CNN卷积神经网络原理、ResNet残差网络从零实现，解决深度网络梯度消失问题
- NLP序列建模基础：循环神经网络（RNN）、LSTM/GRU门控机制、长序列依赖建模详解

### 2. NLP核心基础模块
- 文本表示技术全链路：从one-hot、词袋模型到Word2Vec、GloVe等经典词向量方法，详解文本数字化的核心逻辑

### 3. Transformer核心模块
- 注意力机制全解析：自注意力、多头注意力、掩码注意力的原理与实现
- Transformer完整架构：Encoder-Decoder结构、残差连接、层归一化、位置编码全拆解
- 配套实战：Transformer从零实现代码，可直接用于文本生成、翻译等基础任务

### 4. 大模型核心架构模块
- 预训练模型范式：预训练任务设计、预训练数据处理、大规模预训练核心逻辑
- 主流开源大模型解析：LLaMA2架构全拆解、核心设计思路与推理实战
- 前沿大模型架构：MoE（混合专家模型）核心原理、路由机制、并行训练逻辑，配套可运行的MoE完整工程

### 5. 大模型高效微调（PEFT）模块
- PEFT技术全综述：低秩适配、前缀微调、prompt tuning等主流高效微调方法对比
- LoRA核心详解：低秩适配的数学原理、参数设计、适配场景、优缺点全解析
- 工业级实战：基于Hugging Face Peft库的LoRA实战、通义千问Qwen2.5大模型全流程微调实战

### 6. 大模型人类对齐模块
- RLHF技术全链路详解：监督微调（SFT）、奖励模型训练、强化学习微调三步核心流程
- 主流对齐算法：DPO（直接偏好优化）、PPO算法原理与落地细节
- 配套实战：基于LLaMA-Factory的DPO对齐全流程实战，可直接复现开源大模型的人类对齐效果

### 7. 大模型推理优化与分布式训练模块
- 模型量化技术：INT4/INT8量化原理、量化对精度与性能的影响、主流量化方案对比
- 量化实战：大模型量化全流程实操，实现端侧/服务端轻量化部署
- 分布式训练框架：DeepSpeed核心功能、零冗余优化（ZeRO）、显存优化、大规模训练加速方案详解