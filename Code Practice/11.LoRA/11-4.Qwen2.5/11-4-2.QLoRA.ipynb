{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装依赖（如环境已具备可跳过）\n",
        "%pip install -q -U transformers datasets peft bitsandbytes accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, glob, json, datetime, math\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft.utils import TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING\n",
        "\n",
        "# 基础配置\n",
        "checkpoint_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "artifacts_dir = \"./checkpoints\"\n",
        "data_dir = \"./data\"\n",
        "max_seq_len = 2048\n",
        "seed_num = 42\n",
        "\n",
        "os.makedirs(artifacts_dir, exist_ok=True)\n",
        "\n",
        "# 选择最新训练集（wukong_dataset_*.jsonl）\n",
        "jsonl_files = sorted(glob.glob(os.path.join(data_dir, \"wukong_dataset_*.jsonl\")), key=os.path.getmtime, reverse=True)\n",
        "\n",
        "train_jsonl = jsonl_files[0]\n",
        "print(f\"using dataset: {train_jsonl}\")\n",
        "\n",
        "# 加载数据集\n",
        "train_set = load_dataset(\"json\", data_files=train_jsonl, split=\"train\")\n",
        "train_set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载分词器（Qwen2.5 特点：使用 chat template）\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_id, trust_remote_code=True)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token, tokenizer.eos_token, tokenizer.pad_token_id, tokenizer.eos_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4bit 量化（QLoRA）\n",
        "compute_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint_id,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"cuda:0\",\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.gradient_checkpointing_enable()\n",
        "# k-bit 训练准备（关键，否则反向无 grad）\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LoRA 配置（根据 peft 映射获取 Qwen2 的推荐 target_modules）\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TRANSFORMERS_MODELS_TO_LORA_TARGET_MODULES_MAPPING[\"qwen2\"],\n",
        ")\n",
        "peft_model = get_peft_model(base_model, lora_cfg)\n",
        "# 训练前确保输入需要梯度（配合 k-bit 预处理）\n",
        "peft_model.enable_input_require_grads()\n",
        "peft_model.config.use_cache = False\n",
        "peft_model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 构造监督样本：使用 Qwen 对话模板，并仅对 assistant 段落计算 loss\n",
        "from datasets import Dataset\n",
        "\n",
        "def format_sample_for_qwen(record):\n",
        "    instr = (record.get(\"instruction\") or \"\").strip()\n",
        "    ans = (record.get(\"output\") or \"\").strip()\n",
        "    if not instr or not ans:\n",
        "        return {\"input_ids\": [], \"labels\": []}\n",
        "\n",
        "    msgs_no_assist = [\n",
        "        {\"role\": \"system\", \"content\": \"你是《黑神话：悟空》领域助手，回答准确、简明。\"},\n",
        "        {\"role\": \"user\", \"content\": instr},\n",
        "    ]\n",
        "    # prompt（包含 assistant 起始标记）\n",
        "    prompt_ids = tokenizer.apply_chat_template(\n",
        "        msgs_no_assist,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    msgs_full = msgs_no_assist + [{\"role\": \"assistant\", \"content\": ans}]\n",
        "    full_ids = tokenizer.apply_chat_template(\n",
        "        msgs_full,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=False,\n",
        "        return_tensors=None,\n",
        "    )\n",
        "\n",
        "    # 截断到 max_seq_len\n",
        "    full_ids = full_ids[:max_seq_len]\n",
        "    # 计算分界位置\n",
        "    cut = min(len(prompt_ids), len(full_ids))\n",
        "    labels = [-100] * cut + full_ids[cut:]\n",
        "\n",
        "    return {\"input_ids\": full_ids, \"labels\": labels}\n",
        "\n",
        "proc_train = train_set.map(format_sample_for_qwen, remove_columns=train_set.column_names)\n",
        "proc_train = proc_train.filter(lambda x: len(x[\"input_ids\"]) > 0)\n",
        "proc_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据整理器（按批次 padding，保持 -100 标签）\n",
        "from typing import List, Dict\n",
        "\n",
        "class QwenSftCollator:\n",
        "    def __init__(self, pad_id: int, max_length: int = 2048, ignore_id: int = -100):\n",
        "        self.pad_id = pad_id\n",
        "        self.max_length = max_length\n",
        "        self.ignore_id = ignore_id\n",
        "\n",
        "    def __call__(self, features: List[Dict]):\n",
        "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
        "        max_len = min(max_len, self.max_length)\n",
        "        input_ids, labels = [], []\n",
        "        for f in features:\n",
        "            ids = f[\"input_ids\"][:max_len]\n",
        "            lbs = f[\"labels\"][:max_len]\n",
        "            pad = max_len - len(ids)\n",
        "            if pad > 0:\n",
        "                ids = ids + [self.pad_id] * pad\n",
        "                lbs = lbs + [self.ignore_id] * pad\n",
        "            input_ids.append(torch.tensor(ids, dtype=torch.long))\n",
        "            labels.append(torch.tensor(lbs, dtype=torch.long))\n",
        "        return {\"input_ids\": torch.stack(input_ids), \"labels\": torch.stack(labels)}\n",
        "\n",
        "collator = QwenSftCollator(pad_id=tokenizer.pad_token_id, max_length=max_seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练参数与 Trainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "now_tag = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "run_dir = os.path.join(artifacts_dir, f\"qwen25_wukong_lora_{now_tag}\")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=run_dir,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=4,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_ratio=0.03,\n",
        "    logging_steps=1,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    optim=\"adamw_torch\",\n",
        "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
        "    fp16=not (torch.cuda.is_available() and torch.cuda.is_bf16_supported()),\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=args,\n",
        "    train_dataset=proc_train,\n",
        "    data_collator=collator,\n",
        ")\n",
        "run_dir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 开始训练并保存 LoRA 适配器\n",
        "train_output = trainer.train()\n",
        "print(train_output)\n",
        "\n",
        "peft_model.save_pretrained(run_dir)\n",
        "tokenizer.save_pretrained(run_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 推理测试：参考数据集选择两条问题进行生成\n",
        "peft_model.eval()\n",
        "\n",
        "TEST_QUERIES = [\n",
        "    \"我该怎么成为天命人？\",\n",
        "    \"如何获得并合成出云棍？\",\n",
        "]\n",
        "\n",
        "@torch.no_grad()\n",
        "def infer_one(question: str) -> str:\n",
        "    msgs = [\n",
        "        {\"role\": \"system\", \"content\": \"你是《黑神话：悟空》领域助手，回答准确、简明。\"},\n",
        "        {\"role\": \"user\", \"content\": question},\n",
        "    ]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        msgs,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    input_ids = input_ids.to(peft_model.device)\n",
        "    gen_ids = peft_model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "    out_ids = gen_ids[0, input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(out_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "for q in TEST_QUERIES:\n",
        "    ans = infer_one(q)\n",
        "    print(f\"Q: {q}\\nA: {ans}\\n\" + \"-\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "peft",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
