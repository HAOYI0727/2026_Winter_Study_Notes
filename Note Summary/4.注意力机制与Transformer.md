# 第四章 注意力机制与Transformer

<p align="right">2026.02.04</p>

# 1. Seq2Seq架构

## 1.1. 核心定位

（1）解决问题：针对**多对多（非对齐）** 序列转换任务（输入/输出序列长度不等、无严格对齐），典型场景：机器翻译、文本摘要、对话系统等，弥补了普通RNN/LSTM无法处理此类任务的缺陷。

（2）核心思想：借鉴“**编码器-解码器（Encoder-Decoder）**”架构，模拟人类翻译逻辑：先完整理解输入序列（**编码器**），生成语义概要；再基于语义概要逐词生成输出序列（**解码器**）。

## 1.2. Seq2Seq架构拆解
### 1.2.1. 对比自编码器
| 维度 | 自编码器 | Seq2Seq |
|---|---|---|
| 核心目标 | 数据重构（输出≈输入）| 序列转换（输出≠输入）|
| 应用场景 | 降维、去噪、特征学习 | 跨序列转换（翻译/摘要）|

### 1.2.2. 编码器（Encoder）

（1）核心任务：将整个输入序列压缩为**上下文向量C**（输入序列的“语义概要”）。

（2）实现方式：
- **基础结构：RNN/LSTM/GRU（常用LSTM）**；
- **计算过程**：逐时间步处理输入词元$x_1,x_2,...,x_T$，更新隐藏状态（LSTM含隐藏状态+细胞状态）；
- **上下文向量生成**：通常取最后一个时间步的状态（LSTM为$(h_T,c_T)$），也可融合所有时间步状态；
- **优化**：可使用双向RNN，提升上下文表示的丰富度。

### 1.2.3. 解码器（Decoder）

（1）核心任务：基于上下文向量C，**自回归生成**输出序列$y_1,y_2,...,y_{T'}$。

（2）实现流程：
1. **初始化**：用上下文向量C初始化解码器初始状态（LSTM为$(h'_0,c'_0)$）；
2. **自回归生成**：
   - 第一步：输入`<SOS>`（句子起始符）+ 初始状态，生成$y_1$；
   - 后续步：输入上一步生成的$y_{t-1}$ + 上一步状态，生成$y_t$；
   - 终止条件：生成`<EOS>`（句子终止符）或达到最大长度；
3. **约束**：为满足因果性，解码器通常用单向RNN。

（3）本质：解码器是**条件语言模型**：以上下文向量C为条件，“预测下一个词元”，无编码器时退化为decoder-only模型（如GPT）。

### 1.2.4. 关键实现细节

（1）**词嵌入层**
- 独立嵌入：源/目标语言词汇表独立（如英译中）；
- 共享嵌入：词汇表重叠（如文本摘要），减少参数、挖掘语言关联。

（2）**上下文向量C**的使用策略
| 策略 | 实现方式 | 优点 | 缺点  |
|---|---|---|---|
| 作为解码器初始状态 | C适配后赋值给解码器初始隐藏状态 | 逻辑直观、符合认知 | 长序列易遗忘初始信息 |
| 作为解码器每步输入 | 与词嵌入向量拼接/相加后输入RNN | 每步提醒全局信息 | 静态向量，无法解决“对齐”问题  |

（3）**损失计算**
- 核心损失：**交叉熵损失**（计算预测概率分布与真实词元的差异）；
- 优化：忽略`<PAD>`（填充符）的损失，避免填充干扰梯度；
- 特殊词元：`<PAD>`（对齐长度）、`<SOS>`（起始）、`<EOS>`（终止）、`<UNK>`（未知词）。

## 1.3. 训练与推理模式

### 1.3.1. 训练：教师强制（Teacher Forcing）

（1）**核心逻辑**：解码器每步输入不用模型预测值，而是**直接用真实标签值**，避免误差累积，提升收敛速度。

（2）**灵活策略**：可设置“**教师强制概率**”，随机切换“**真实标签/模型预测**”作为输入，平衡训练效率与泛化性。

### 1.3.2. 推理：自回归生成

（1）**核心逻辑：无真实标签**，解码器以`<SOS>`为起点，逐步将上一步输出作为下一步输入，直到生成`<EOS>`。

（2）**效率优化：缓存上一步的隐藏状态**，避免重复计算（仅用最后一个词元+上一步状态做单步计算）。

（3）解码策略
| 策略 | 逻辑 | 优缺点 |
|---|---|---|
| 贪心搜索 | 每步选概率最高的词元 | 高效但易出局部最优解 |
| 束搜索 | 每步保留多个候选序列，选全局最优 | 效果更好但计算成本更高  |

## 1.4. PyTorch代码实现核心

### 1.4.1. 核心模块

（1）编码器
```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=hidden_size
        )
        self.rnn = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=False
        )

    def forward(self, x):
        # x shape: (batch_size, seq_length)
        embedded = self.embedding(x)
        # 返回最终的隐藏状态和细胞状态作为上下文
        _, (hidden, cell) = self.rnn(embedded)
        return hidden, cell
```

（2）解码器（单步计算，适配高效推理）

```python
class Decoder(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers):
        super(Decoder, self).__init__()
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=hidden_size
        )
        self.rnn = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(in_features=hidden_size, out_features=vocab_size)

    def forward(self, x, hidden, cell):
        # x shape: (batch_size)，只包含当前时间步的token
        x = x.unsqueeze(1) # -> (batch_size, 1)

        embedded = self.embedding(x)
        # 接收上一步的状态 (hidden, cell)，计算当前步
        outputs, (hidden, cell) = self.rnn(embedded, (hidden, cell))

        predictions = self.fc(outputs.squeeze(1)) # -> (batch_size, vocab_size)
        return predictions, hidden, cell
```

（3）Seq2Seq包装器（整合训练逻辑）
- 初始化输出张量，存储每步预测；
- 编码器生成上下文向量，解码器以`<SOS>`为起点循环解码；
- 按概率切换“教师强制”模式（真实标签/模型预测作为下一步输入）。

```Python
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = src.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.fc.out_features

        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)
        hidden, cell = self.encoder(src)

        # 第一个输入是 <SOS>
        input = trg[:, 0]

        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[:, t, :] = output

            # 决定是否使用 Teacher Forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            # 如果 teacher_force，下一个输入是真实值；否则是模型的预测值
            input = trg[:, t] if teacher_force else top1

        return outputs
```

### 1.4.2. 高效推理实现

- 核心：缓存解码器状态，避免重复计算；
- **贪心解码**：每步仅输入上一个生成词元+上一步状态，单步计算并更新状态，直到生成`<EOS>`。

```Python
# ... 在 Seq2Seq 类中 ...
    def greedy_decode(self, src, max_len=12, sos_idx=1, eos_idx=2):
        """推理模式下的高效贪心解码。"""
        self.eval()
        with torch.no_grad():
            hidden, cell = self.encoder(src)
            trg_indexes = [sos_idx]
            for _ in range(max_len):
                # 1. 输入只有上一个时刻的词元
                trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(self.device)
                
                # 2. 解码一步，并传入上一步的状态
                output, hidden, cell = self.decoder(trg_tensor, hidden, cell)
                
                # 3. 获取当前步的预测，并更新状态用于下一步
                pred_token = output.argmax(1).item()
                trg_indexes.append(pred_token)
                if pred_token == eos_idx:
                    break
        return trg_indexes
```

### 1.4.3. 解码器变体（上下文向量每步输入）
- 改动1：RNN输入维度=词嵌入维度+上下文向量维度；
- 改动2：每步将词嵌入与上下文向量拼接后输入RNN；
- 初始状态：解码器初始状态设为零，上下文仅通过输入注入。

```Python
class DecoderAlt(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers):
        super(DecoderAlt, self).__init__()
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=hidden_size
        )
        # 主要改动 1: RNN的输入维度是 词嵌入+上下文向量
        self.rnn = nn.LSTM(
            input_size=hidden_size + hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(in_features=hidden_size, out_features=vocab_size)

    def forward(self, x, hidden_ctx, hidden, cell):
        x = x.unsqueeze(1)
        embedded = self.embedding(x)

        # 主要改动 2: 将上下文向量与当前输入拼接
        # 这里简单地取编码器最后一层的 hidden state 作为上下文代表
        context = hidden_ctx[-1].unsqueeze(1).repeat(1, embedded.shape[1], 1)
        rnn_input = torch.cat((embedded, context), dim=2)

        # 解码器的初始状态 hidden, cell 在第一步可设为零；之后需传递并更新上一步状态
        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))
        predictions = self.fc(outputs.squeeze(1))
        return predictions, hidden, cell
```

## 1.5. 应用与局限
### 1.5.1. 应用场景（Encoder-Decoder泛化性）
| 任务类型  | 编码器  | 解码器 |
|---|---|---|
| 语音识别 | 音频特征提取模型 | 文本生成模型 |
| 图像描述生成 | CNN（视觉特征提取）| 文本生成模型 |
| 文本到语音（TTS）| 文本处理模型 | 音频波形生成模型 |
| 问答系统 | 文章+问题编码 | 答案生成模型 |

### 1.5.2. 核心局限：信息瓶颈
（1）**问题本质**：编码器需将任意长度输入序列压缩为**固定长度**上下文向量C，长序列**易丢失关键信息**；解码器每步依赖同一个静态C，无法“**针对性关注**”输入序列的特定部分。

（2）解决方案：催生**注意力机制**：允许解码器生成每个词元时，动态计算输入序列的权重分布，重点关注对应部分，解决对齐问题，为**Transformer**奠定基础。

# 2. 注意力机制

## 2.1. 背景

（1）标准Seq2Seq架构将源序列所有信息压缩为**固定长度上下文向量C**，存在两大缺陷：**长序列易丢失开头关键信息；解码器生成每个词元时，无法选择性关注输入不同部分**（如对联生成需针对性关注对应词）。

（2）注意力机制的提出，正是为了打破这种固定向量的限制，实现动态聚焦。

## 2.2. 原理

（1）设计思想：**从“一言以蔽之（固定向量）”→“择其要者而观之（动态加权）”**，模拟人类认知的选择性聚焦：解码器生成第t个词元时，不再依赖固定C，而是对编码器所有隐藏状态动态分配注意力权重，加权求和生成专属当前步的上下文向量$C_t$。

（2）**动态加权公式**（核心）：

$$C_t = \sum_{j=1}^{T_x} \alpha_{tj} h_j$$

- $C_t$：解码第t步的动态上下文向量；
- $\alpha_{tj}$：解码第t步时，输入第j个词的注意力权重（总和为1、非负）；
- $h_j$：编码器第j个词的隐藏状态；
- 固定对齐策略（如$C_1=h_1$）是该公式的特例（$\alpha_{11}=1$，其余为0）。

（3）**注意力权重的计算**：

1. **计算相似度**：解码器上一状态$h'_{t-1}$（Query）与编码器各隐藏状态$h_j$（Key）计算关联分数$e_{tj} = \text{score}(h'_{t-1}, h_j)$

2. **归一化权重**：Softmax将分数转为概率分布（权重）$\alpha_{tj} = \text{softmax}(e_{tj}) = \frac{\exp(e_{tj})}{\sum_{i=1}^{T_x} \exp(e_{ti})}$

3. **加权求和**：权重对编码器隐藏状态（Value）加权，生成$C_t = \sum_{j=1}^{T_x} \alpha_{tj} h_j$

（4）**关键：高效的打分函数（缩放点积）**
$$\text{score}(h'_{t-1}, h_j) = \frac{{h'_{t-1}}^T \cdot h_j}{\sqrt{d_k}}$$
> $d_k$：Key向量维度，除以$\sqrt{d_k}$可缓解高维向量点积方差过大导致的梯度消失问题，稳定训练。

## 2.3. 通用范式：QKV（查询-键-值）

（1）核心概念
| 组件 | 含义 | Seq2Seq中的对应对象 |
|---|---|---|
| **Query**（Q） | **当前解码需求/意图** | 解码器上一时刻隐藏状态$h'_{t-1}$ |
| **Key**（K） | **输入信息的“标签/索引”** | 编码器各隐藏状态$h_j$ |
| **Value**（V） | **输入信息的实际内容** | 编码器各隐藏状态$h_j$（基础版K=V） |

（2）公式（**缩放点积注意力**）：
$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

> 可对Q/K/V先做独立线性变换（提升表达能力），再计算注意力。

## 2.4. PyTorch实现

### 2.4.1. 编码器

```python
class Encoder(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers):
        super(Encoder, self).__init__()
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=hidden_size
        )
        self.rnn = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True  # 使用双向LSTM
        )
        self.fc = nn.Linear(hidden_size * 2, hidden_size)

    def forward(self, x):
        embedded = self.embedding(x)
        outputs, (hidden, cell) = self.rnn(embedded)
        
        # 将双向RNN的输出通过线性层降维，使其与解码器维度匹配
        outputs = torch.tanh(self.fc(outputs))

        return outputs, hidden, cell*
```
1. 用**双向LSTM**提升上下文捕获能力；
2. 返回**所有**时间步输出（作为K/V）；
3. 线性层将**双向**输出降维（匹配解码器维度）

### 2.4.2. 注意力模块

（1）**无参数版**：直接缩放点积计算权重
```python
class AttentionSimple(nn.Module):
    """1: 无参数的注意力模块"""
    def __init__(self, hidden_size):
        super(AttentionSimple, self).__init__()
        # 确保缩放因子是一个 non-learnable buffer
        self.register_buffer("scale_factor", torch.sqrt(torch.FloatTensor([hidden_size])))

    def forward(self, hidden, encoder_outputs):
        # hidden shape: (num_layers, batch_size, hidden_size)
        # encoder_outputs shape: (batch_size, src_len, hidden_size)
        
        # Q: 解码器最后一层的隐藏状态
        query = hidden[-1].unsqueeze(1)  # -> (batch, 1, hidden)
        # K/V: 编码器的所有输出
        keys = encoder_outputs  # -> (batch, src_len, hidden)

        # energy shape: (batch, 1, src_len)
        energy = torch.bmm(query, keys.transpose(1, 2)) / self.scale_factor
        
        # attention_weights shape: (batch, src_len)
        return torch.softmax(energy, dim=2).squeeze(1)
```

（2）**带参数版（交叉注意力）**：线性层+可学习向量v，自主学习Q-K关联
```Python
class AttentionParams(nn.Module):
    """2: 带参数的注意力模块"""
    def __init__(self, hidden_size):
        super(AttentionParams, self).__init__()
        self.attn = nn.Linear(hidden_size * 2, hidden_size)
        self.v = nn.Parameter(torch.rand(hidden_size))

    def forward(self, hidden, encoder_outputs):
        src_len = encoder_outputs.shape[1]
        hidden_last_layer = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)
        
        energy = torch.tanh(self.attn(torch.cat((hidden_last_layer, encoder_outputs), dim=2)))
        attention = torch.sum(self.v * energy, dim=2)
        
        return torch.softmax(attention, dim=1)
```

### 2.4.3. 解码器

```python
class DecoderWithAttention(nn.Module):
    def __init__(self, vocab_size, hidden_size, num_layers, attention_module):
        super(DecoderWithAttention, self).__init__()
        self.attention = attention_module
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=hidden_size
        )
        self.rnn = nn.LSTM(
            input_size=hidden_size * 2,  # 输入维度是 词嵌入(hidden_size) + 上下文向量(hidden_size)
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, vocab_size)

    def forward(self, x, hidden, cell, encoder_outputs):
        embedded = self.embedding(x.unsqueeze(1))

        # 1. 计算注意力权重
        # a shape: [batch, src_len]
        a = self.attention(hidden, encoder_outputs).unsqueeze(1)
        # 2. 计算上下文向量
        context = torch.bmm(a, encoder_outputs)
        # 3. 将上下文向量与当前输入拼接
        rnn_input = torch.cat((embedded, context), dim=2)
        # 4. 传入RNN解码
        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))
        # 5. 预测输出
        predictions = self.fc(outputs.squeeze(1))
        
        return predictions, hidden, cell
```

1. 集成注意力模块；2. 输入维度变为“**词嵌入+上下文向量**”（hidden_size*2）；3. **逐时间步循环解码**（每步计算注意力）

### 2.4.4. Seq2Seq包装层

```python
class Seq2Seq(nn.Module):
    """带注意力的Seq2Seq"""
    def __init__(self, encoder, decoder, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size = src.shape[0]
        trg_len = trg.shape[1]
        trg_vocab_size = self.decoder.fc.out_features
        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)

        encoder_outputs, hidden, cell = self.encoder(src)

        # 适配Encoder(双向)和Decoder(单向)的状态维度
        hidden = hidden.view(self.encoder.rnn.num_layers, 2, batch_size, -1).sum(dim=1)
        cell = cell.view(self.encoder.rnn.num_layers, 2, batch_size, -1).sum(dim=1)

        input = trg[:, 0]
        for t in range(1, trg_len):
            # 在循环的每一步，都将 encoder_outputs 传递给解码器
            # 这是 Attention 机制能够"回顾"整个输入序列的关键
            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)
            outputs[:, t, :] = output
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[:, t] if teacher_force else top1
            
        return outputs
```

1. 适配双向编码器与单向解码器的状态维度；2. 循环解码时，每步传递编码器所有输出，支持动态注意力计算

## 2.5. 类型

（1）**Soft Attention vs Hard Attention**
| 类型 | 特点 | 优缺点 |
|-----|-----|-----|
| **Soft Attention** | 对所有输入位置分配0~1的**连续权重，加权求和** | 优点：端到端可微、训练简单；缺点：长序列计算开销大（O(N²)） |
| **Hard Attention** | 仅选择一个**最相关的输入位置**（权重非0即1） | 优点：计算量小；缺点：离散不可微，需强化学习训练 |

（2）**Global Attention vs Local Attention**
| 类型 | 特点 | 适用场景 |
|-----|-----|-----|
| **Global Attention** | 关注编码器**所有隐藏状态**（同Soft Attention） | 常规长度序列、算力充足场景 |
| **Local Attention** | 折中方案：**预测聚焦位置→定义局部窗口→仅在窗口内计算Soft Attention** | 长序列、低延迟/资源受限场景 |

## 2.6. 价值
1. **性能提升**：打破固定上下文向量的信息瓶颈，长序列任务性能大幅提升；
2. **可解释性**：注意力权重矩阵可可视化，清晰展示模型生成某输出词时关注的输入词（词对齐）。

# 3. Transformer 核心知识点笔记

## 3.1. 背景
（1）**提出背景**：2017年Google《Attention Is All You Need》论文提出，抛弃RNN/卷积网络，完全基于**注意力机制**构建，解决了传统Seq2Seq的信息瓶颈、长序列效率低、长距离依赖丢失问题。

（2）**核心优势：并行计算提升训练效率，有效捕捉长距离依赖，为BERT、GPT等预训练模型奠定架构基础。**

## 3.2. 自注意力机制（Self-Attention）

### 3.2.1. 核心定义
（1）**单个序列内部的注意力计算**，Query（Q）、Key（K）、Value（V）均来自同一输入序列，目的是捕捉序列内部依赖，重构词元的上下文表示。

（2）与交叉注意力对比：
| 类型 | 信息来源 | 核心目标 |
|---|---|---|
| **交叉注意力** | **Q来自解码器，K/V来自编码器** | 对齐、整合两个不同序列信息 |
| **自注意力** | **Q/K/V均来自同一输入序列**  | 理解、重构单个序列内部依赖 |

### 3.2.2. 计算流程（QKV范式）
1. **生成Q/K/V**：输入词嵌入$x_i$分别与可学习权重矩阵$W^Q/W^K/W^V$相乘，投影到注意力空间：$q_i = x_i W^Q$、$k_i = x_i W^K$、$v_i = x_i W^V$
2. **计算注意力分数**：$score(i,j) = q_i \cdot k_j$（第$i$个词元对第$j$个词元的相关性）
3. **缩放与归一化**：分数除以$\sqrt{d_k}$（缓解梯度消失），再经Softmax归一化：$\alpha_{ij} = softmax\left(\frac{q_i \cdot k_j}{\sqrt{d_k}}\right)$
4. **加权求和**：用权重$\alpha_{ij}$对所有$v_j$加权，生成上下文表示$z_i$：$z_i = \sum_j \alpha_{ij} v_j$

### 3.2.3. 并行化实现
（1）矩阵运算形式：$Z = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$（Q/K/V为整个序列的矩阵，一次性完成所有词元计算）。

（2）PyTorch核心实现：**通过线性层生成Q/K/V，矩阵乘法替代循环，实现并行计算**。

```Python
class SelfAttention(nn.Module):
    """自注意力模块"""
    def __init__(self, hidden_size):
        super(SelfAttention, self).__init__()
        self.hidden_size = hidden_size
        self.q_linear = nn.Linear(hidden_size, hidden_size)
        self.k_linear = nn.Linear(hidden_size, hidden_size)
        self.v_linear = nn.Linear(hidden_size, hidden_size)
        
    def forward(self, x):
        q = self.q_linear(x)
        k = self.k_linear(x)
        v = self.v_linear(x)
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.hidden_size)
        attention_weights = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, v)
        
        return context
```

## 3.3. 多头注意力机制（Multi-Head Attention）

（1）**核心思想**：**并行运行多个独立的自注意力“头”**，每个头有专属$W^Q_i/W^K_i/W^V_i$，学习不同视角的上下文关系（如语法依赖、语义关联），提升模型表达能力。**将隐藏维度$hidden\_size$均分至各头（如512维分8头，每头64维），保持计算量不变。**

（2）**计算流程**：
1. 并行计算$h$个头的自注意力输出$Z_0-Z_{h-1}$；
2. 拼接所有头的输出；
3. 经输出权重矩阵$W^O$投影回原始维度，得到最终输出。

（3）PyTorch实现：
```Python
class MultiHeadSelfAttention(nn.Module):
    """多头自注意力模块"""
    def __init__(self, hidden_size, num_heads):
        super(MultiHeadSelfAttention, self).__init__()
        assert hidden_size % num_heads == 0, "hidden_size 必须能被 num_heads 整除"
        
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        
        self.q_linear = nn.Linear(hidden_size, hidden_size)
        self.k_linear = nn.Linear(hidden_size, hidden_size)
        self.v_linear = nn.Linear(hidden_size, hidden_size)
        self.wo = nn.Linear(hidden_size, hidden_size)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        
        q = self.q_linear(x)
        k = self.k_linear(x)
        v = self.v_linear(x)
        
        # 拆分多头
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 并行计算注意力
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attention_weights = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, v)
        
        # 合并多头结果
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
        
        # 输出层
        output = self.wo(context)
        
        return output
```

- **拆分多头**：将Q/K/V维度重塑为`[batch, num_heads, seq_len, head_dim]`，实现多头发并行计算；
- **合并多头**：转置+contiguous+view恢复原始维度，最后经线性层融合输出。

## 3.4. Transformer 整体架构（Encoder-Decoder）

### 3.4.1. 编码器（Encoder）
（1）结构：N层（原论文N=6）编码器层堆叠，每层包含：**多头自注意力层 → 残差连接+层归一化 → 位置前馈网络 → 残差连接+层归一化**。

（2）关键特性：**双向自注意力**，擅长自然语言理解（NLU）任务（如BERT）。

```Python
import torch
import torch.nn as nn
import math
# 导入组件
from .attention import MultiHeadAttention
from .ffn import FeedForward
from .norm import LayerNorm
from .pos import PositionalEncoding

class EncoderLayer(nn.Module):
    def __init__(self, dim, n_heads, hidden_dim, dropout=0.1):
        super().__init__()
        # 多头自注意力子层
        self.attention = MultiHeadAttention(dim, n_heads, dropout)
        self.attention_norm = LayerNorm(dim)
        # 前馈网络子层
        self.feed_forward = FeedForward(dim, hidden_dim, dropout)
        self.ffn_norm = LayerNorm(dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, mask=None):
        # 子层 1：自注意力
        _x = x
        x = self.attention(x, x, x, mask)  # Q=K=V=x
        x = self.attention_norm(_x + self.dropout(x))
        
        # 子层 2：前馈网络
        _x = x
        x = self.feed_forward(x)
        x = self.ffn_norm(_x + self.dropout(x))
        
        return x
```

### 3.4.2. 解码器（Decoder）
（1）结构：N层解码器层堆叠，每层包含：**带掩码的多头自注意力层 → 残差+归一化 → 交叉注意力层 → 残差+归一化 → 位置前馈网络 → 残差+归一化**。

（2）核心特性：
1. **掩码自注意力**：通过“未来掩码”实现自回归，生成第$t$个词元时仅依赖前$t-1$个；
2. **交叉注意力**：Q来自解码器，K/V来自编码器，连接编码和解码过程；
3. **自回归生成**：推理时逐个生成词元（GPT类模型基于Decoder-Only架构）。

```Python
class DecoderLayer(nn.Module):
    def __init__(self, dim, n_heads, hidden_dim, dropout=0.1):
        super().__init__()
        # 1. 带掩码的自注意力
        self.self_attention = MultiHeadAttention(dim, n_heads, dropout)
        self.self_attn_norm = LayerNorm(dim)
        # 2. 交叉注意力
        self.cross_attention = MultiHeadAttention(dim, n_heads, dropout)
        self.cross_attn_norm = LayerNorm(dim)
        # 3. 前馈网络
        self.feed_forward = FeedForward(dim, hidden_dim, dropout)
        self.ffn_norm = LayerNorm(dim)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x, enc_output, src_mask, tgt_mask):
        # 子层 1：带掩码的自注意力
        _x = x
        x = self.self_attention(x, x, x, tgt_mask)
        x = self.self_attn_norm(_x + self.dropout(x))
        
        # 子层 2：交叉注意力（Q 来自解码器，K/V 来自编码器输出）
        _x = x
        x = self.cross_attention(x, enc_output, enc_output, src_mask)
        x = self.cross_attn_norm(_x + self.dropout(x))
        
        # 子层 3：前馈网络
        _x = x
        x = self.feed_forward(x)
        x = self.ffn_norm(_x + self.dropout(x))
        
        return x
```

### 3.4.3. 核心组件

（1）**位置前馈网络（FFN）**
- 结构：升维（$W_1$，通常4倍隐藏维）→ 激活（ReLU/GELU）→ 降维（$W_2$）：$FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2$
- 作用：为每个位置提供非线性特征变换。

```Python
import torch.nn as nn

class FeedForward(nn.Module):
    def __init__(self, dim, hidden_dim, dropout=0.1):
        super().__init__()
        self.w1 = nn.Linear(dim, hidden_dim)  # 升维
        self.w2 = nn.Linear(hidden_dim, dim)  # 降维
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # 线性变换 -> ReLU -> Dropout -> 线性变换
        return self.w2(self.dropout(torch.relu(self.w1(x))))
```

（2）**残差连接+层归一化（Add & Norm）**
- **残差连接**：$y = x + \text{Sublayer}(x)$，解决深度网络退化，保证梯度回传；
- **层归一化（LayerNorm）**：对每个词元的特征向量标准化（均值0、方差1），引入可学习参数$\gamma/\beta$，适配NLP变长序列（优于BatchNorm）；
- 两种范式：Post-LN（原论文）、Pre-LN（现代模型，训练更稳定）。

```Python
import torch
import torch.nn as nn

class LayerNorm(nn.Module):
    """
    层归一化 (Layer Normalization)
    公式: y = (x - mean) / sqrt(var + eps) * gamma + beta
    """
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        # 可学习参数 gamma (缩放) 和 beta (偏移)
        # nn.Parameter 会被自动注册为模型参数
        self.gamma = nn.Parameter(torch.ones(dim))
        self.beta = nn.Parameter(torch.zeros(dim))

    def forward(self, x):
        # x: [batch_size, seq_len, dim]
        # 在最后一个维度 (dim) 上计算均值和方差，keepdim=True 保持维度以便进行广播计算
        mean = x.mean(-1, keepdim=True)
        # unbiased=False 使用有偏估计 (分母为 N)，与 PyTorch 默认行为一致
        var = x.var(-1, keepdim=True, unbiased=False)
        # 归一化
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        # 缩放和平移
        return self.gamma * x_norm + self.beta
```

（3）**位置编码（Positional Encoding）**
- 解决自注意力“位置无关性”问题，将位置信息注入词嵌入：$input_{embedding} = token_{embedding} + positional_{encoding}$。
- 两种实现方式：
  | 类型 | 实现方式 | 优势 |
  |---|---|---|
  | **可学习位置编码** | nn.Embedding层学习各位置向量 | 适配Encoder-only模型（如BERT）|
  | **三角函数固定编码** | 正弦/余弦函数生成固定向量 | 无需训练，可外推长序列 |
- 进阶方向：现代模型多用**相对位置编码（如RoPE）**，建模词元相对距离，适配超长序列。

```Python
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    """
    正弦位置编码
    """
    def __init__(self, dim, max_seq_len=5000):
        super().__init__()
        
        # 1.创建一个足够长的 PE 矩阵 [max_seq_len, dim]
        pe = torch.zeros(max_seq_len, dim)
        # 2.生成位置索引 [0, 1, ..., max_seq_len-1] -> [max_seq_len, 1]
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        # 3.计算分母中的 div_term: 10000^(2i/dim) = exp(2i * -log(10000)/dim)
        # 这种对数变换的计算方式在数值上更稳定
        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))
        
        # 4.填充 PE 矩阵——偶数维度用 sin，奇数维度用 cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        # 5.增加 batch 维度: [1, max_seq_len, dim] 以便广播
        pe = pe.unsqueeze(0)
        
        # 6.注册为 buffer
        # register_buffer 的作用是告诉 PyTorch：'pe' 是模型状态的一部分，会随模型保存和加载 (state_dict)；不是模型参数 (Parameter)，优化器更新时不会更新它。
        self.register_buffer('pe', pe)

    def forward(self, x):
        # 截取与输入序列长度对应的位置编码并相加
        # x.size(1) 是 seq_len
        # self.pe 的形状是 [1, max_seq_len, dim]，切片后会自动广播到 batch_size
        x = x + self.pe[:, :x.size(1), :]
        return x
```

（4）注意力机制与注意力掩码
| 类型 | 应用场景 | 核心作用 |
|---|---|---|
| **因果掩码** | **解码器**自注意力层 | 屏蔽**未来词元**，保证自回归生成 |
| **填充掩码** | **编码器/解码器**所有注意力层 | 屏蔽**Padding位**，避免无效计算和噪声 |

```Python
import torch
import torch.nn as nn
import math

class MultiHeadAttention(nn.Module):
    def __init__(self, dim, n_heads, dropout=0.1):
        super().__init__()
        self.dim = dim
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        
        # 定义 Wq, Wk, Wv 矩阵——使用一个大的线性层一次性计算所有头的 Q, K, V
        self.wq = nn.Linear(dim, dim)
        self.wk = nn.Linear(dim, dim)
        self.wv = nn.Linear(dim, dim)
        
        # 最终输出的线性层 Wo
        self.wo = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, q, k, v, mask=None):
        batch_size = q.size(0)
        
        # 1. 线性投影
        # [batch, seq_len, dim] -> [batch, seq_len, dim]
        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)
        
        # 2. 分头 (Split Heads)
        # 变换形状: [batch, seq_len, n_heads, head_dim] 
        # 然后转置: [batch, n_heads, seq_len, head_dim] 以便并行计算
        q = q.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)
        
        # 3. 计算缩放点积注意力 (Scaled Dot-Product Attention)
        # scores: [batch, n_heads, seq_len, seq_len]
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # 4. 应用掩码 (Masking)
        if mask is not None:
            # mask == 0 的位置被填充为负无穷，Softmax 后变为 0
            scores = scores.masked_fill(mask == 0, float('-inf'))
            
        # 5. Softmax 与加权求和
        attn_weights = torch.softmax(scores, dim=-1)
        
        if self.dropout is not None:
             attn_weights = self.dropout(attn_weights)
             
        # context: [batch, n_heads, seq_len, head_dim]
        context = torch.matmul(attn_weights, v)
        
        # 6. 合并多头 (Concat Heads)
        # [batch, n_heads, seq_len, head_dim] -> [batch, seq_len, n_heads, head_dim]
        # -> [batch, seq_len, dim]
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.dim)
        
        # 7. 输出层投影
        output = self.wo(context)
        
        return output
```

（5）KV缓存（推理优化）
- 核心原理：**缓存已生成词元的K/V向量**，推理时仅计算当前词元的Q，复用历史K/V，将计算复杂度从$O(T^2)$降至$O(T)$，提升生成效率。
- 注意点：显存占用随序列长度线性增长，需关注多层多头场景下的显存开销。

## 3.5. 代码实践核心
### 3.5.1. 模块化设计
拆分核心组件到独立文件，保证复用性：
```
transformer/
├── src/
│   ├── transformer.py  # Encoder/Decoder/整体框架
│   ├── attention.py    # 多头注意力
│   ├── ffn.py          # 前馈网络
│   ├── norm.py         # 层归一化
│   ├── pos.py          # 位置编码
└── main.py             # 模型组装与前向传播
```

### 3.5.2. 核心实现思路
- 自顶向下：先定义Transformer整体框架，再实现各组件；
- 关键细节：维度匹配（残差连接要求输入输出维度一致）、多头拆分/合并、掩码的正确应用。
```Python
class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, 
                 dim=512, n_heads=8, n_layers=6, 
                 hidden_dim=2048, max_seq_len=5000, dropout=0.1):
        super().__init__()

        self.dim = dim
        # 1. 嵌入层与位置编码
        # src_embedding: 将源语言序列映射为向量 (Encoder输入)
        self.src_embedding = nn.Embedding(src_vocab_size, dim)
        # tgt_embedding: 将目标语言序列映射为向量 (Decoder输入)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, dim)
        self.pos_encoder = PositionalEncoding(dim, max_seq_len)
        self.dropout = nn.Dropout(dropout)
        
        # 2. 编码器与解码器堆叠
        # 使用 ModuleList 来存储层列表，支持按索引访问和自动注册参数
        self.encoder_layers = nn.ModuleList([
            EncoderLayer(dim, n_heads, hidden_dim, dropout) for _ in range(n_layers)
        ])
        self.decoder_layers = nn.ModuleList([
            DecoderLayer(dim, n_heads, hidden_dim, dropout) for _ in range(n_layers)
        ])
        
        # 3. 输出头
        self.output = nn.Linear(dim, tgt_vocab_size)

    def _init_parameters(self):
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)

    def generate_mask(self, src, tgt):
        # src_mask: [batch, 1, 1, src_len]，pad token 假设为 0
        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)
        
        # tgt_mask: [batch, 1, tgt_len, tgt_len]，结合 pad mask 和 causal mask
        tgt_len = tgt.size(1)
        tgt_pad_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)  # [batch, 1, 1, tgt_len]
        tgt_subsequent_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()
        tgt_mask = tgt_pad_mask & tgt_subsequent_mask.unsqueeze(0)
        return src_mask, tgt_mask

    def encode(self, src, src_mask):
        x = self.src_embedding(src) * math.sqrt(self.dim)
        x = self.pos_encoder(x)
        x = self.dropout(x)
        for layer in self.encoder_layers:
            x = layer(x, src_mask)
        return x

    def decode(self, tgt, enc_output, src_mask, tgt_mask):
        x = self.tgt_embedding(tgt) * math.sqrt(self.dim)
        x = self.pos_encoder(x)
        x = self.dropout(x)
        for layer in self.decoder_layers:
            x = layer(x, enc_output, src_mask, tgt_mask)
        return x

    # 前向传播
    def forward(self, src, tgt):
        # 1. 生成掩码 (Padding Mask & Causal Mask)
        src_mask, tgt_mask = self.generate_mask(src, tgt)
        # 2. 编码器前向传播
        enc_output = self.encode(src, src_mask)
        # 3. 解码器前向传播
        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)
        # 4. 输出 Logits
        return self.output(dec_output)
        return logits
```