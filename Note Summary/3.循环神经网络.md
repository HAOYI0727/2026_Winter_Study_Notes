# 第三章 循环神经网络

<p align="right">2026.01.31</p>

# 1. RNN

## 1.1. 背景

（1） 核心问题：需将词向量序列融合为能体现**语序、上下文**的定长文本向量，用于后续任务（如意图识别）。

（2）传统方法的局限性
| 方法 | 核心问题 |
|------|----------|
| **词向量求和/平均** | 忽略语序、无权重区分（关键信息与虚词同等对待） |
| **全连接网络（FCN）** | 孤立处理每个词元，无上下文依赖，未捕获序列特征 |
| **CNN（一维卷积）** | 感受野固定，难以捕捉长距离依赖，窗口大小难适配所有句子 |

## 1.2. RNN 核心：引入“记忆”的序列处理

### 1.2.1. 基本定义

循环神经网络（RNN）通过“**隐藏状态**”传递历史记忆，处理每个时间步时**融合当前输入+上一步记忆**，解决上下文依赖问题。

### 1.2.2. 核心结构

（1）**公式**：$h_t = \tanh(U x_t + W h_{t-1} + b)$（简化版无偏置：$h_t = \tanh(U x_t + W h_{t-1})$）
  - $x_t$：当前时间步输入（词向量）
  - $h_{t-1}$：上一步隐藏状态（历史记忆）
  - $U/W$：共享权重矩阵（所有时间步参数复用）
  - $h_t$：当前隐藏状态（融合当前输入+历史记忆）
  
（2）**关键机制**：**权重共享** → 适配任意长度序列，减少参数；**隐藏状态传递** → 捕获上下文。

### 1.2.3. 工作原理
以“播放周杰伦的《稻香》”为例：

（1）分词+词嵌入得到 $x_1$（播放）、$x_2$（周杰伦）、$x_3$（的）、$x_4$（《稻香》）；

（2）初始隐藏状态 $h_0 = 0$，逐时间步计算：
   - $h_1 = \tanh(Ux_1 + Wh_0)$（含“播放”信息）
   - $h_2 = \tanh(Ux_2 + Wh_1)$（含“播放+周杰伦”信息）
   - $h_3 = \tanh(Ux_3 + Wh_2)$（含“播放周杰伦的”信息）
   - $h_4 = \tanh(Ux_4 + Wh_3)$（含整句信息）

（3）最终 $h_4$ 作为“文本向量”送入分类器，实现序列特征编码。

### 1.2.4. 静态→动态的飞跃
（1）静态（Type）：Word2Vec 词向量固定，仅代表词的通用含义；

（2）动态（Token）：RNN 隐藏状态 $h_t$ 随上下文变化（如“周杰伦”在“播放周杰伦”/“我喜欢周杰伦”中表示不同）；

（3）补充：ELMo 普及了“动态词向量”范式，而 RNN 是动态表示的核心机制。

## 1.3. RNN 实现与验证

### 1.3.1. 手动实现（NumPy）
核心步骤：
1. 初始化零向量隐藏状态 $h_{prev}$；
2. 逐时间步遍历输入，计算 $h_t = \tanh(x_t @ U + h_{prev} @ W)$；
3. 更新 $h_{prev}$ 为当前 $h_t$，保存所有时间步结果。

```Python
def manual_rnn_numpy(x_np, U_np, W_np):
    B_local, T_local, _ = x_np.shape
    # 初始化 h_0 为零向量
    h_prev = np.zeros((B_local, H), dtype=np.float32)
    
    steps = []
    # 按时间步循环
    for t in range(T_local):
        x_t = x_np[:, t, :]
        # 核心公式实现
        h_t = np.tanh(x_t @ U_np + h_prev @ W_np)
        steps.append(h_t)
        h_prev = h_t # 更新状态
        
    return np.stack(steps, axis=1), h_prev
```

### 1.3.2. PyTorch 封装实现（nn.RNN）

| 参数 | 含义 |
|------|------|
| input_size | 输入维度（词嵌入维度） |
| hidden_size | 隐藏状态维度 |
| batch_first | 输入形状是否为 (B, T, E)（默认 (T, B, E)） |
| bidirectional | 是否双向 RNN |
| bias | 是否使用偏置项 |
- 权重对应：`weight_ih_l0` = $U^T$，`weight_hh_l0` = $W^T$

```Python
def pytorch_rnn_forward(x, U, W):
    rnn = nn.RNN(
        input_size=E,
        hidden_size=H,
        num_layers=1,
        nonlinearity='tanh',
        bias=False,
        batch_first=True,
        bidirectional=False,
    )
    with torch.no_grad():
        # PyTorch 内部存放的是转置后的权重
        rnn.weight_ih_l0.copy_(U.T)
        rnn.weight_hh_l0.copy_(W.T)
    y, h_n = rnn(x)
    return y, h_n.squeeze(0)
```

## 1.4. 双向 RNN（BiRNN）

（1）解决问题：单向 RNN 仅利用前文，BiRNN **同时捕捉前文+后文上下文**（如“苹果”需结合“味道不错”/“股票大涨”判断含义）。

（2）核心结构：由正向 RNN（从左到右）和反向 RNN（从右到左）组成；
- 每个时间步输出：$h_t = [\overrightarrow{h_t} ; \overleftarrow{h_t}]$（拼接正向/反向隐藏状态）；
- 输出维度：2×hidden_size（正向/反向各 hidden_size）。

（3）优势与局限：无需人工设置时间延迟窗口，协同训练正向/反向权重；但未解决长距离依赖，无法实时预测（需完整序列）。

## 1.5. RNN 训练：随时间反向传播（BPTT）

（1）核心逻辑：将 RNN 沿时间展开为深层前馈网络，总损失 $L = \sum_{t=1}^T L_t$，梯度计算需沿时间反向追溯：
$$\frac{\partial L}{\partial W} = \sum_{t=1}^{T} \sum_{k=1}^{t} \frac{\partial L_t}{\partial h_t} \cdot \prod_{i=k+1}^{t} \frac{\partial h_i}{\partial h_{i-1}} \cdot \frac{\partial h_k}{\partial W}$$

（2）核心问题：**梯度消失/爆炸**
| 问题 | 原因 | 解决方式 |
|------|------|----------|
| 梯度消失 | $W$ 范数<1 或激活函数导数<1，连乘后梯度指数级衰减 | 无直接解法（后续 LSTM/GRU 解决） |
| 梯度爆炸 | $W$ 范数>1，梯度指数级增长 | 梯度裁剪（超过阈值缩放） |

（3）**长距离依赖问题**：梯度消失导致模型**无法捕捉远距离词的依赖**（如“弼马温”与“嘲讽”的关联）；正向传播中，早期信息被逐步“稀释”，模型存在“近期偏置”。

（4）其他局限：单向性（BiRNN 缓解但未解决本质），需 LSTM/GRU 等门控结构突破。

# 2. LSTM与GRU

## 2.1. 背景

常规RNN仅通过单一隐藏状态$h_t$传递信息，梯度反向传播时因权重矩阵连乘易出现**梯度消失/爆炸**，**无法处理长距离依赖问题**。LSTM和GRU通过**门控机制**赋予网络信息取舍能力，缓解该问题。

## 2.2. LSTM（长短期记忆网络）

### 2.2.1. 核心设计：双轨状态+三门控
| 状态/门控 | 作用 | 关键特性 |
|----------|------|----------|
| **细胞状态$c_t$（长期记忆）** | 信息高速公路，传递长期记忆 | 仅按元素加权/相加，**无矩阵连乘，梯度衰减慢** |
| **隐藏状态$h_t$（短期记忆）** | 当前时间步输出，依赖$c_t$ | 与RNN隐藏状态功能类似 |
| **遗忘门$f_t$** | 决定**丢弃**$c_{t-1}$中多少信息 | $\sigma$激活，输出∈(0,1)，$f_t=\sigma(W_f·[h_{t-1},x_t]+b_f)$ |
| **输入门$i_t$**| 决定向$c_t$**写入**多少新信息 | $\sigma$激活，$i_t=\sigma(W_i·[h_{t-1},x_t]+b_i)$ |
| **输出门$o_t$** | 决定从$c_t$**输出**多少信息到$h_t$ | $\sigma$激活，$o_t=\sigma(W_o·[h_{t-1},x_t]+b_o)$ |


### 2.2.2. LSTM计算流程（t时刻）
1. **遗忘门**：计算$f_t$，筛选需保留的旧细胞状态；
2. **输入门+候选记忆**：$i_t$筛选新信息，$\tilde{c}_t=\tanh(W_c·[h_{t-1},x_t]+b_c)$生成候选记忆；
3. **更新细胞状态**：$c_t = (f_t \odot c_{t-1}) + (i_t \odot \tilde{c}_t)$（$\odot$为按元素乘法）；
4. **输出门+隐藏状态**：$h_t = o_t \odot \tanh(c_t)$，$h_t$传递至下一时刻并作为当前输出。

### 2.2.3. 缓解长距离依赖的原理

- 梯度沿细胞状态反向传播时：$\frac{\partial L}{\partial c_k} = \frac{\partial L}{\partial c_t} \odot (f_t \odot f_{t-1} \odot \dots \odot f_{k+1})$，仅乘以遗忘门连乘值（无权重矩阵）。
- 若$f_t≈1$，梯度可近乎无衰减传递，将长距离依赖转化为“可学习”问题（模型自主调整$f_t$）;
- 框架中会将遗忘/输入/候选/输出门的权重矩阵拼接，合并矩阵乘法操作，利用GPU并行提升效率。

### 2.2.4. 手动实现LSTM
（1）**初始化**: 两个零向量 $h_prev$ 和 $c_prev$，分别作为处理序列开始前的“**短期记忆**”和“**长期记忆**”；

（2）**逐帧处理**: 遍历序列中的每一个时间步，对输入进行处理；

（3）**核心计算**: 在循环内部，首先计算**遗忘门$f_t$**，决定从旧的细胞状态 $c_{prev}$ 中忘记多少信息；然后计算输入门 $i_t$ 和候选记忆 $c_{tilde_t}$，准备要写入的新信息；随后通过公式 $c_t = f_t * c_{prev} + i_t * c_{tilde_t}$，结合遗忘和记忆操作，得到新的细胞状态 $c_t$；最后计算输出门 $o_t$，并结合 $tanh(c_t)$ 生成新的隐藏状态 $h_t$；

（4）**状态更新**: 在每一步计算结束后，执行 $h_{prev}, c_{prev} = h_t, c_t$，将当前计算出的状态传递给下一个时间步。

```Python
def manual_lstm_numpy(x_np, weights):
    U_f, W_f, U_i, W_i, U_c, W_c, U_o, W_o = weights
    B_local, T_local, _ = x_np.shape
    h_prev = np.zeros((B_local, H), dtype=np.float32)
    c_prev = np.zeros((B_local, H), dtype=np.float32)
    
    steps = []
    # 按时间步循环
    for t in range(T_local):
        x_t = x_np[:, t, :]
        
        # 1. 遗忘门
        f_t = sigmoid(x_t @ U_f + h_prev @ W_f)
        
        # 2. 输入门与候选记忆
        i_t = sigmoid(x_t @ U_i + h_prev @ W_i)
        c_tilde_t = np.tanh(x_t @ U_c + h_prev @ W_c)
        
        # 3. 更新细胞状态
        c_t = f_t * c_prev + i_t * c_tilde_t
        
        # 4. 输出门与隐藏状态
        o_t = sigmoid(x_t @ U_o + h_prev @ W_o)
        h_t = o_t * np.tanh(c_t)
        
        steps.append(h_t)
        h_prev, c_prev = h_t, c_t
        
    outputs = np.stack(steps, axis=1)
    return outputs, h_prev, c_prev
```


## 2.3. GRU（门控循环单元）

（1）核心改进（简化LSTM）：**合并细胞状态与隐藏状态**：仅保留$h_t$传递信息；**简化门控**：2个门替代LSTM的3个门。

（2）GRU计算流程（t时刻）
| 步骤 | 公式 | 作用 |
|------|------|------|
| **重置门$r_t$** | $r_t=\sigma(W_r·[h_{t-1},x_t]+b_r)$ | 控制计算候选记忆时忽略多少旧信息 |
| **更新门$z_t$**| $z_t=\sigma(W_z·[h_{t-1},x_t]+b_z)$ | **融合遗忘门+输入门功能**，控制旧信息保留/新信息写入比例 |
| **候选记忆$\tilde{h}_t$** | $\tilde{h}_t=\tanh(W_h·[r_t \odot h_{t-1},x_t]+b_h)$ | 受重置门调控的新状态候选 |
| **最终隐藏状态$h_t$** | $h_t = z_t \odot h_{t-1} + (1-z_t) \odot \tilde{h}_t$ | $z_t≈1$保留旧信息，$z_t≈0$保留新信息 |

（3）特点：结构更简单、参数量更少、训练更易，能自适应捕捉不同时间尺度依赖。

## 2.4. LSTM常见变体

### 2.4.1. 窥孔连接（Peephole Connections）
（1）改进：门控单元可直接访问细胞状态（遗忘/输入门看$c_{t-1}$，输出门看$c_t$）；
（2）公式示例：$f_t = \sigma(W_f·[h_{t-1},x_t] + V_f \odot c_{t-1} + b_f)$（$V_f$为对角权重矩阵）；
（3）优势：处理计时/计数类任务性能更优。

### 2.4.2. 耦合的输入/遗忘门（CIFG）
（1）改进：$i_t=1-f_t$，遗忘旧信息与写入新信息耦合；
（2）细胞状态更新：$c_t = (f_t \odot c_{t-1}) + ((1-f_t) \odot \tilde{c}_t)$；
（3）优势：减少参数量，不降低性能，逻辑更直观。
