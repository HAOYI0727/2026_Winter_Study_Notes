# 模型量化实战

<p align="right">2026.02.22</p>

## 1. 量化核心概念

### 1.1. 定义

量化是**用更少信息表示数据，利用模型参数冗余性，降低参数数值精度**（如FP16→INT4），在**减少显存占用、提升计算速度**的同时，**尽可能保留模型性能**，实现“瘦身不降智”。

### 1.2. 核心价值
- **降低显存开销**：INT8量化显存占用较FP16减半，INT4仅为1/4，使大模型可在消费级显卡运行；
- **提升推理速度**：减少数据量降低内存带宽压力，在LLM推理（内存受限场景）中直接提升Token生成速度。

### 1.3. 精度与显存基础

|精度类型|占用字节|特点|
|---|---|---|
|FP32|4|训练默认精度，推理无需高精度|
|**FP16/BF16**|2|FP16易溢出；BF16牺牲小数精度换数值范围，大模型训练主流|
|INT8|1|显存占用为FP16的1/2|
|INT4|0.5|显存占用为FP16的1/4|

### 1.4. 显存估算

- **通用公式**：**权重显存占用 ≈ 模型参数量 × 每参数占用字节数（近似1GB≈10⁹Bytes）**；
- **实际需求**：需预留20%~30%显存给KV Cache、激活值、框架开销（如7B INT4模型推荐显存≥6GB）。

## 2. 主流量化方案（Transformers集成）

|方案|核心定位|核心原理/特点|优势|
|---|---|---|---|
|**GPTQ**|推理部署/PTQ|基于近似二阶信息（**海森矩阵**）的一次性权重量化，最小化量化前后激活值平方误差；**分块量化+误差补偿**|千亿级模型量化精度损失小，速度快|
|**AWQ**|推理部署/PTQ|激活感知权重量化，按**激活值幅值**识别重要权重，通过**等价缩放**降低量化误差（全INT量化接近混合精度性能）|端侧友好，LLaMA/Llama-2系列量化PPL更低|
|**BitsAndBytes(BNB)**|推理+低显存微调|解决“**离群值**”问题，LLM.int8()混合精度分解：99.9%常规值INT8计算，0.1%离群值FP16计算；支持**NF4类型**（QLoRA依赖）|几乎无损精度，支持4-bit微调|

关键细节：
- **GPTQ**：利用海森矩阵逆补偿量化误差，延迟批量更新提升GPU利用率，Cholesky分解保证数值稳定性；
- **AWQ**：核心是寻找最优缩放因子，放大重要权重通道降低相对量化误差，全INT量化性能接近混合精度；
- **BNB**：针对大模型（≥6.7B）涌现的离群特征，动态混合精度计算，避免精度崩塌，是QLoRA的核心依赖。

## 3. Qwen2.5量化实战（基于llmcompressor）

### 3.1. 环境准备

安装llmcompressor库（支持GPTQ/AWQ等算法，与Hugging Face生态深度集成，适配vLLM部署）。

### 3.2. 通用量化流程

1. **初始化环境：指定模型、设备**；

```Python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from llmcompressor.modifiers.quantization import GPTQModifier
from llmcompressor import oneshot

# 基础配置
base_model_id = "Qwen/Qwen2.5-1.5B-Instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"
```

2. **定义量化策略：设置精度（如W4A16）、量化目标（Linear层）、忽略层（lm_head，避免输出乱码）**；

```Python
# 量化后模型输出目录
gptq_out_dir = "models/qwen2.5-1.5b-instruct-gptq-llmc"

# 定义 GPTQ 量化策略
gptq_recipe = [
    GPTQModifier(
        scheme="W4A16",      # 权重 4bit，激活保持 16bit
        targets="Linear",    # 只量化线性层
        ignore=["lm_head"],  # 保持输出头的高精度，避免性能损失
    ),
]
```
- `scheme="W4A16"`：权重4bit量化，激活16bit计算，平衡显存与精度；
- `ignore=["lm_head"]`：输出头对精度敏感，量化易导致性能崩塌；
- `num_calibration_samples=128`：128~512个样本足以计算准确统计信息。

3. **执行One-Shot量化：加载模型，用校准数据集（如open_platypus）计算统计信息，应用量化算法**；

```Python
oneshot(
    model=base_model_id,
    dataset="open_platypus",       # 使用公开数据集进行校准
    recipe=gptq_recipe,            # 传入定义好的量化策略
    output_dir=gptq_out_dir,
    max_seq_length=2048,
    num_calibration_samples=128,   # 128个样本通常足够计算准确的统计信息
)
```

4. **验证：加载量化模型，测试推理效果**。

```Python
# 加载 GPTQ 量化后的检查点做推理

gptq_tokenizer = AutoTokenizer.from_pretrained(gptq_out_dir, trust_remote_code=True)
if gptq_tokenizer.pad_token_id is None:
    gptq_tokenizer.pad_token = gptq_tokenizer.eos_token

gptq_model = AutoModelForCausalLM.from_pretrained(
    gptq_out_dir,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True,
)
gptq_model.eval()

# 打印 tokenizer 的特殊 token 信息，确保 pad_token 设置正确
gptq_tokenizer.pad_token, gptq_tokenizer.eos_token, gptq_tokenizer.pad_token_id, gptq_tokenizer.eos_token_id
```

### 3.3. GPTQ vs AWQ实战差异

|维度|GPTQ|AWQ|
|---|---|---|
|校准依赖|海森矩阵（需少量校准样本）|激活值幅值（需网格搜索缩放因子）|
|耗时|快（确定性数学计算）|慢（迭代搜索最优缩放因子）|
|核心优势|千亿模型量化速度快|端侧友好，vLLM加速支持好|
|误差补偿|二阶信息补偿|等价缩放平滑离群值误差|

> 总结
> 1. 量化是大模型落地的关键技术，不同方案适配不同场景（GPTQ/AWQ侧重推理，BNB兼顾推理与微调）；
> 2. 量化需兼顾精度与效率，合理选择量化精度、校准数据集，规避关键层（如lm_head）量化；
> 3. 模型规模越大，量化技术（如GPTQ/BNB）的精度保护效果越显著，INT4量化可在极低显存占用下保留核心性能。