# 第五章 预训练模型

<p align="right">2026.02.07</p>

# 1. BERT
## 1.1. 核心定位

（1）**基础架构**：基于 Transformer 的**编码器**结构（区别于 GPT 用解码器），主打**深度双向语义理解**，适用于语言理解任务，无改造时不适用于文本生成。

（2）**核心优势**：**通过自注意力机制实现真正的深度双向**，能捕捉更长距离、更复杂的语义依赖，且并行计算效率远超 RNN/LSTM（Bi-LSTM 仅为浅层双向拼接）。

## 1.2. 训练范式：预训练 + 微调（迁移学习）
| 阶段 | 目标 | 操作 |
|---|---|---|
| **预训练** | 学习**通用**语言规律（语法、语义、上下文依赖）| 在海量通用语料上完成**无监督任务训练**，得到预训练模型（参数已初始化）|
| **微调** | 适配**具体**下游任务 | 加载预训练模型，新增任务相关输出层，在**小量任务数据上训练**（仅微调或全量训练）|

## 1.3. 架构细节

### 1.3.1. 模型规模
| 模型 | 层数 | 隐藏层大小 | 注意力头数 | 总参数量 |
|---|---|---|---|---|
| BERT-Base | 12 | 768 | 12 | ~1.1亿 |
| BERT-Large | 24 | 1024 | 16 | ~3.4亿 |

### 1.3.2. 输入表示

公式：$Input_{embedding} = Token_{embedding} + Position_{embedding} + Segment_{embedding}$

| 嵌入类型 | 作用 | 关键细节 |
|---|---|---|
| **词元嵌入** | 表示单个**词元**，解决**未登录词** | 采用 **WordPiece** 分词（中文 bert-base-chinese 以单字为主+少量常用词）|
| **片段嵌入** | 区分输入中的不同**句子**（适配句子对任务）| 句子A/句子B 分别加专属嵌入向量 |
| **位置嵌入** | 补充**序列位置信息**（Transformer 无内置位置感知）| 可学习的嵌入表，决定最大输入长度（BERT-Base 为 512 词元）|

### 1.3.3. 特殊词元
| 特殊 Token | 全称 | 核心作用 |
|---|---|---|
| [CLS] | Classification | 序列开头，其输出作为句子级聚合表示（文本分类任务核心）|
| [SEP] | Separator | 分隔句子（单句加在末尾，句子对加在两句间+末尾）|
| [MASK] | Mask | 预训练阶段掩码词元，用于MLM任务 |
| [PAD] | Padding | 补齐序列长度，注意力计算时被屏蔽 |
| [UNK] | Unknown | 替换词表外的未知词元  |

## 1.4. 预训练任务

### 1.4.1. 掩码语言模型（MLM）
（1）**目标**：通过“**完形填空**”学习上下文依赖，生成动态词向量。

（2）**策略**：随机选 15% 词元，按 80% 替换为 [MASK]、10% 替换为随机词、10% 保持原词执行预测，缓解预训练/微调数据差异。

（3）**局限与优化**：单字/子词掩码割裂完整词义 → **全词掩码（WWM）优化**。

### 1.4.2. 下一句预测（NSP）
（1）**目标**：学习句子间逻辑关系，二分类任务（判断B是否是A的真实下一句）。

（2）**实现**：基于 [CLS] 输出向量做预测；后续研究（RoBERTa/ALBERT）发现大规模预训练下移除/替换该任务效果更好，但原始BERT中该任务对问答/NLI等任务有增益。

## 1.5. 应用与实践

### 1.5.1. 下游任务微调方式
| 任务类型 | 核心操作 |
|---|---|
| **文本分类** | 提取 [CLS] 输出向量，接全连接分类层，微调模型 |
| **词元分类**（NER/分词）| 提取所有词元输出向量（shape: [batch_size, seq_len, hidden_size]），接共享权重的全连接层训练 |
| **问答任务** | 问题+段落作为句子对输入，训练预测答案在段落中的起止位置 |

### 1.5.2. 关键实践技巧
（1）**最大长度限制**：标准模型最大输入 512 词元（含特殊Token，单句有效长度 510，句子对需扣3个特殊Token）。

（2）**特征提取优化**：底层捕捉词法/语法，高层捕捉语义，可拼接/相加最后几层向量提升效果。

（3）**工具生态**：Hugging Face transformers 库（统一API、海量预训练模型、完善文档）。

### 1.5.3. 代码实战

```Python
import torch
import os
from transformers import AutoTokenizer, AutoModel

# 1. 环境和模型配置
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' # 可选：设置镜像
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "bert-base-chinese"
texts = ["我来自中国", "我喜欢自然语言处理"]

# 2. 加载模型和分词器
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).to(device)
model.eval()

print("\n--- BERT 模型结构 ---")
print(model)

# 3. 文本预处理
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(device)

# 打印 Tokenizer 的完整输出，以理解其内部结构
print("\n--- Tokenizer 输出 ---")
for key, value in inputs.items():
    print(f"{key}: \n{value}\n")

# 4. 模型推理
with torch.no_grad():
    outputs = model(**inputs)

# 5. 提取特征
last_hidden_state = outputs.last_hidden_state
sentence_features_pooler = getattr(outputs, "pooler_output", None)

# (1) 提取句子级别的特征向量 ([CLS] token)
sentence_features = last_hidden_state[:, 0, :]

# (2) 提取第一个句子的词元级别特征
first_sentence_tokens = last_hidden_state[0, 1:6, :]


print("\n--- 特征提取结果 ---")
print(f"句子特征 shape: {sentence_features.shape}")
if sentence_features_pooler is not None:
    print(f"pooler_output shape: {sentence_features_pooler.shape}")
print(f"第一个句子的词元特征 shape: {first_sentence_tokens.shape}")

```

1. **加载分词器+模型**（AutoTokenizer/AutoModel.from_pretrained）；
2. **文本预处理**：自动添加特殊Token、padding/truncation、生成attention_mask/token_type_ids；
3. **推理**：禁用梯度计算（torch.no_grad()），输入模型得到last_hidden_state；
4. **特征提取**：句子特征取 [:,0,:]（[CLS]），词元特征切片提取对应位置。

## 1.6. BERT 与 Transformer（Encoder）核心区别
| 维度 | Transformer（Encoder）| BERT |
|---|---|---|
| 模型规模 | 论文中 N=6 层 | 提供Base（12层）/Large（24层）等规模，更宽/更深                       |
| 输入表示 | 仅**词嵌入+固定正余弦位置编码** | **词嵌入+可学习位置嵌入+片段嵌入**，适配句子对任务 |
| 训练范式 | 无预训练/微调范式 | **预训练（MLM+NSP）+ 微调的迁移学习范式** |
| 位置编码 | 固定正余弦函数 | **可学习的位置嵌入表（决定最大输入长度）**|
| 应用定位 | 通用特征提取器 | 面向语言理解的专用预训练模型，有成熟的特殊Token/任务适配设计 |

# 2. GPT

## 2.1. 核心定位与设计理念

### 2.1.1. 技术路线

（1）基于 **Transformer 解码器** 构建，专注**语言生成**，区别于BERT（Transformer编码器，专注语言理解）

（2）核心特性：**自回归（Auto-Regressive）预测下一个词元，仅能关注前文信息，无法看到未来内容。**

### 2.1.2. 预训练任务
（1）**因果语言模型（CLM）**：**最大化预测下一个词元的概率**，公式为 $P(x_1, ..., x_T) = \prod_{t=1}^{T} P(x_t | x_1, ..., x_{t-1})$。

（2）目标：学习语言深层规律（语法、事实、逻辑）。

### 2.1.3. 发展阶段：从微调到提示
| 模型 | 核心能力 | 关键创新/特点 |
|---|---|---|
| GPT-1 | **有监督微调** | 任务特定输入变换（特殊分隔符统一不同任务输入格式）；预训练+有监督微调范式 |
| GPT-2 | **零样本学习** | 语言模型即多任务学习者；Pre-activation（LayerNorm移到子层输入端）；上下文窗口扩至1024 |
| GPT-3 | **上下文学习（In-context Learning）** | 1750亿参数；零样本/单样本/少样本学习；无需微调，仅通过提示词激发能力 |

## 2.2. 架构解析

### 2.2.1. 基础结构
（1）核心：堆叠Transformer**解码器层**，移除原始解码器的“交叉注意力层”（无编码器交互）。

（2）单解码器层组成：**掩码多头自注意力（实现单向自回归）+ 位置前馈网络（与BERT编码器一致，非线性变换）**。

### 2.2.2. 关键架构演进
| 特性 | GPT-1 | GPT-2/3 | 备注 |
|---|---|---|---|
| 特殊Token | - | `<\|endoftext\|>` | 分隔文档/示例、标记文本结束 |
| 归一化方式 | Post-Norm | **Pre-Norm** | Pre-Norm缓解深层梯度问题，支持训练更深模型 |
| 嵌入层 | 词元+位置 | 词元+位置 | 无片段嵌入（区别于BERT） |
| 上下文窗口 | 512 | GPT-2:1024；GPT-3:2048 | 决定输入文本最大长度 |
| 分词方式 | - | **字节级BPE** | 无UNK词，非英文编码效率低 |

### 2.2.3. GPT vs BERT 核心差异
| 特性 | BERT（Encoder） | GPT（Decoder） |
|---|---|---|
| 注意力 | **双向自注意力** | **掩码单向自注意力** |
| 预训练任务 | **掩码LM+下一句预测** | **因果LM（预测下一个词）** |
| 输入表示 | **词元+位置+片段** | **词元+位置** |
| 应用模式 | **微调（更新权重）** | **提示（Prompt，不更新权重）** |
| 适用场景 | **语言理解**（分类、NER、抽取式问答） | **语言生成**（写作、对话、生成式任务） |

## 2.3. 代码实战

### 2.3.1. 自回归生成原理
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# 1. 环境配置
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
model.eval()

# 2. 英文示例
prompt_en = "I like eating fried"
input_ids_en = tokenizer(prompt_en, return_tensors="pt")['input_ids'].to(device)

print(f"英文输入: '{prompt_en}'")
print(f"被编码为 {input_ids_en.shape[1]} 个 token: {tokenizer.convert_ids_to_tokens(input_ids_en[0])}")
print("开始为英文逐个 token 生成...")

generated_ids_en = input_ids_en
with torch.no_grad():
    for i in range(5): # 只生成 5 个 token 作为示例
        outputs = model(generated_ids_en)
        next_token_logits = outputs.logits[:, -1, :]
        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
        new_token = tokenizer.decode(next_token_id[0])
        print(f"第 {i+1} 步, 生成 token: '{new_token.strip()}'")
        generated_ids_en = torch.cat([generated_ids_en, next_token_id], dim=1)

full_decoded_text_en = tokenizer.decode(generated_ids_en[0], skip_special_tokens=True)
print(f"\n英文最终生成结果: \n'{full_decoded_text_en}'\n")


# 3. 中文示例
prompt_zh = "我喜欢吃炸"
input_ids_zh = tokenizer(prompt_zh, return_tensors="pt")['input_ids'].to(device)

print(f"中文输入: '{prompt_zh}'")
print(f"被编码为 {input_ids_zh.shape[1]} 个 token: {tokenizer.convert_ids_to_tokens(input_ids_zh[0])}")
print("开始为中文逐个 token 生成...")

generated_ids_zh = input_ids_zh
with torch.no_grad():
    for i in range(5): # 只生成 5 个 token 作为示例
        outputs = model(generated_ids_zh)
        next_token_logits = outputs.logits[:, -1, :]
        next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)
        new_token = tokenizer.decode(next_token_id[0])
        print(f"第 {i+1} 步, 生成 token: '{new_token.strip()}'")
        generated_ids_zh = torch.cat([generated_ids_zh, next_token_id], dim=1)

full_decoded_text_zh = tokenizer.decode(generated_ids_zh[0], skip_special_tokens=True)
print(f"\n中文最终生成结果 (出现乱码是正常现象): \n'{full_decoded_text_zh}'")
```
- 核心逻辑：**将每一步生成的token拼接到输入序列，作为下一步输入（贪心搜索：取logits最大的token）。**

### 2.3.2. 分词器（字节级BPE）
（1）优点：**无未知词（UNK）**，任何文本可拆分为基础字节；

（2）缺点：**非英文（如中文）编码效率低**，一个汉字拆分为2-3个字节token，导致序列变长、语义学习困难；

（3）中文生成乱码原因：GPT-2训练语料为英文，仅能按英文统计规律预测字节，不符合中文编码规则。

### 2.3.3. 实用工具
```python
    # pipeline 应用
    from transformers import pipeline

    print("\n\n--- Pipeline 快速演示 (英文) ---")
    generator = pipeline("text-generation", model=model_name, device=device)
    pipeline_outputs = generator("I like eating fried", max_new_tokens=5, num_return_sequences=1)
    print(pipeline_outputs[0]['generated_text'])
```
- **Hugging Face Pipeline：封装分词、推理、解码，一行实现文本生成**

# 3. T5

## 3.1. 核心定位

**T5（Text-to-Text Transfer Transformer）** 回归Transformer经典的**Encoder-Decoder架构**，将所有NLP任务统一为“**文本到文本**”转换问题，兼顾BERT的理解能力与GPT的生成能力。

## 3.2. 核心理念

### 3.2.1. 任务统一范式
**所有任务输入输出均为文本**，通过**任务前缀（Task Prefix）** 区分任务，示例：
| 任务类型 | 输入示例 | 输出示例 |
|---|---|---|
| 翻译 | 翻译成英文: 黑神话悟空真好玩 | Black Myth: Wukong is really fun. |
| 情感分类 | 情感分析: 黑神话悟空真好玩 | 正面                              |
| 摘要 | 摘要: 黑神话悟空是一款...动作角色扮演游戏 | 黑神话悟空是国产3A动作游戏。 |
| 数值回归  | 计算相似度 句子1: ... 句子2: ... | 4.5（文本形式） |

### 3.2.2. 任务前缀与GPT Prompt的区别
| 特性 | **T5 Prefix** | **GPT Prompt** |
|---|---|---|
| 核心用途 | **有监督微调**，多任务学习标记 | **零样本/少样本推理**，理解用户意图 |
| 训练依赖 | **训练时见过前缀**，关联任务参数 | **预训练未见过**，依赖模型泛化能力 |

### 3.2.3. 多任务学习策略
**采用“带有上限的比例混合”：对大数据集设采样上限、提升小数据集采样概率，避免大任务淹没小任务。**

### 3.2.4. 独特预训练目标：Span Corruption
不同于BERT单字掩码、GPT单向预测，适配Encoder-Decoder结构：
- **破坏**：随机选中连续文本片段（Span），替换为**唯一哨兵符**（如`<extra_id_0>`）；
- **重构**：Decoder生成被遮盖的片段；
- **关键参数**：遮盖15%的token，平均片段长度为3时性能最佳；

## 3.3. 架构解析
（1）整体结构：**标准Encoder-Decoder**——Encoder：双向理解输入文本（类BERT）；Decoder：自回归生成输出文本（类GPT）。

（2）关键技术改进
| 改进点 | 核心设计 | 优势 |
|---|---|---|
| **相对位置编码** | 1. **分桶策略**：近距离精确（<8）、远距离对数映射归桶；2. **作为Bias加在Attention Score上，所有层共享参数** | 减少参数量，贴合语言距离特性 |
| **简化版Layer Normalization** | **去除加性偏置，仅缩放激活值** | 减少参数与计算开销 |
| **SentencePiece分词器** | **直接处理原始文本**，空格视为特殊字符，无语言专属规则 | 天然支持多语言，适配大一统理念 |

## 3.4. 代码实战

### 3.4.1. 基础使用（Text-to-Text演示）

```python
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration

# 1. 加载模型
model_name = "t5-small" # 使用最小版本演示
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# 2. 准备输入
# T5 需要明确的任务前缀
input_text_1 = "translate English to German: The house is wonderful."
input_text_2 = "stsb sentence1: The rhino grazed on the grass. sentence2: A rhino is grazing in a field."

# 3. 推理生成
inputs = tokenizer([input_text_1, input_text_2], return_tensors="pt", padding=True)
outputs = model.generate(**inputs)

print(f"输入 1: {input_text_1}")
print(f"输出 1: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

print(f"输入 2: {input_text_2}")
print(f"输出 2: {tokenizer.decode(outputs[1], skip_special_tokens=True)}")
```
- 注意：原版T5无中英翻译训练，需用mT5（支持101种语言）处理中文任务。

### 2. 相对位置编码核心逻辑

```python
import math
import torch

def compute_bias(self, query_length, key_length, device=None):
    """
    计算相对位置偏置矩阵的完整流程
    """
    # 1. 生成位置索引
    # context_position (Query的位置): [0, 1, ..., q_len-1]
    context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]
    # memory_position (Key的位置): [0, 1, ..., k_len-1]
    memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]
    
    # 2. 计算相对距离 (Relative Distance)
    # 矩阵相减，得到 (q_len, k_len) 的相对距离矩阵
    relative_position = memory_position - context_position 

    # 3. 映射到桶 (Bucketing)
    # 调用 _relative_position_bucket 函数，将具体距离映射为 bucket_id
    relative_position_bucket = self._relative_position_bucket(
        relative_position, 
        bidirectional=(not self.is_decoder),
        num_buckets=self.relative_attention_num_buckets,
        max_distance=self.relative_attention_max_distance,
    )

    # 4. 查 Embedding 表 (Lookup)
    # self.relative_attention_bias 是一个可学习的 Embedding 层
    # 根据 bucket_id 查出对应的 bias 值
    values = self.relative_attention_bias(relative_position_bucket)
    
    # 调整形状以适配 Multi-head Attention: (1, n_heads, q_len, k_len)
    values = values.permute([2, 0, 1]).unsqueeze(0)
    return values

def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):
    """
    T5 相对位置编码的核心分桶逻辑
    将相对距离（relative_position）映射为一个桶编号（bucket ID）
    """
    relative_buckets = 0
    
    # 1. 处理双向/单向 Attention
    # 如果是双向 Attention (如 Encoder)，正负距离是不同的桶
    if bidirectional:
        num_buckets //= 2
        # 如果距离 > 0 (Key 在 Query 后面)，桶编号加上总数的一半
        relative_buckets += (relative_position > 0).to(torch.long) * num_buckets
        # 取绝对值，统一处理正负距离
        relative_position = torch.abs(relative_position)
    else:
        # 如果是单向 Attention (如 Decoder)，只考虑过去的距离
        relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))
    
    # 2. 核心分桶逻辑：近距离精确，远距离模糊
    
    # 前一半的桶（max_exact）用于精确匹配近距离
    max_exact = num_buckets // 2
    is_small = relative_position < max_exact

    # 情况1：距离较小 (is_small 为 True)，直接使用距离作为桶编号
    # 例如距离为 1 -> 桶 1; 距离为 5 -> 桶 5
    
    # 情况2：距离较大 (is_small 为 False)，使用对数公式计算桶编号
    # 使用对数函数 log 把很大的距离压缩到剩下的桶里
    relative_position_if_large = max_exact + (
        torch.log(relative_position.float() / max_exact)
        / math.log(max_distance / max_exact)
        * (num_buckets - max_exact)
    ).to(torch.long)
    
    # 防止越界，最大不超过 num_buckets - 1
    relative_position_if_large = torch.min(
        relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)
    )

    # 根据 is_small 的判断，选择使用精确编号还是对数编号
    relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)
    return relative_buckets
```
- **解耦位置与内容**：在每一层 Attention 计算时，直接在这个 N×N 的**注意力分数矩阵上加上一个位置偏置矩阵（Bias）**，让位置信息更直接地作用于注意力权重。
- **参数效率**：通过**分桶（Bucketing）**策略，将无限的距离映射到有限的桶（如 32 个）中，大大减少了参数量。
- **对数映射**：分桶时采用“**近密远疏**”的策略（对数映射），因为人类语言对近距离的语法依赖（如主谓关系）非常敏感，需要精确区分；而对于远距离的语义依赖，只需要知道“大概很远”就足够了。

# 4. Hugging Face 生态与核心库

## 4.1. Hugging Face 生态全景

（1）核心定位：AI 时代的基础设施，构建了覆盖模型开发全生命周期的生态系统，Hugging Face Hub 是 AI 领域的 GitHub，集成模型、数据集、应用演示，支持 Git 版本控制。

（2）核心构成：
| 类别 | 核心组件 | 功能说明 |
|---|---|---|
| 协作平台 | Hugging Face Hub | - **Models**：托管数十万预训练模型（NLP/CV/Audio 等）<br>- **Datasets**：托管公开数据集，支持预览和流式传输<br>- **Spaces**：快速构建 Web 应用展示模型 |
| 核心软件库   | **Transformers** | 生态引擎，提供统一 API 下载/加载/使用预训练模型 |
| | **Tokenizers** | 文本-模型桥梁，Rust 实现 Fast Tokenizers，处理速度极快 |
| | **Datasets** | 数据处理加速器，标准化接口加载/处理数据集，支持内存映射 |
| 辅助工具库   | **Accelerate** | 简化分布式训练，适配 CPU/GPU/TPU 等硬件 |
| | **Evaluate** | 标准化评估框架，提供 Accuracy/F1/BLEU 等数十种评估指标 |
| | **Diffusers** | 专注生成式 AI（如 Stable Diffusion） |
| | **PEFT** | 参数高效微调库，支持 LoRA 等微调技术 |

## 4.2. Transformers 核心库

### 4.2.1. Pipeline：开箱即用
（1）封装“**预处理→模型推理→后处理**”全流程，支持 NLP/跨模态任务

（2）**无需指定模型，仅需指定任务**（如 sentiment-analysis），自动加载默认模型；中文任务建议显式指定中文模型（如 bert-base-chinese）

### 4.2.2. AutoClass：智能加载
（1）核心：**根据 Checkpoint 名称自动推断并加载对应模型架构**（如 AutoModel.from_pretrained("bert-base-chinese") 加载 BertModel）

（2）接口：**from_pretrained**：从 Hub 在线下载/本地目录加载模型；**save_pretrained**：保存模型权重、配置、词表到本地

### 4.2.3. 核心处理流程
| 阶段 | 功能 | 示例代码操作 |
|---|---|---|
| **Tokenizer** | **文本→数字序列**（Input IDs/Attention Mask） | AutoTokenizer.from_pretrained(checkpoint)；tokenizer(text, return_tensors="pt") |
| **Model** | **Tensor→Logits**（未归一化得分） | AutoModelForSequenceClassification.from_pretrained(checkpoint)；model(**inputs**) |
| **Post-processing** | **Logits→人类可读概率/标签** | torch.nn.functional.softmax(logits, dim=-1) 转换为概率分布 |

## 4.3. Datasets 构建数据流水线

### 4.3.1. 核心优势
（1）底层基于 Apache Arrow，支持**内存映射**：无需加载全量数据集到内存，适配 TB 级数据

（2）支持**流式模式**：按需迭代数据，无需下载完整文件

### 4.3.2. 并行预处理

（1）**map 函数：将预处理函数（如 Tokenizer）应用到数据集样本**

（2）优化：**batched=True** 开启批处理，**num_proc=N** 开启多进程并行，提升处理速度

## 4.4. Trainer 与 Evaluate

### 4.4.1. Trainer 训练框架（高度封装）
（1）优势：集成混合精度、梯度累积、分布式训练（底层调用 Accelerate），无需手写 PyTorch 循环

（2）使用步骤：
- **准备组件**：**实例化** Model/Dataset/Tokenizer，**预处理**数据集（如 tokenize）
- **配置参数**：TrainingArguments 定义**批大小、学习率、训练轮数、保存策略**等
- **启动训练**：**实例化 Trainer**，传入模型、参数、数据集，调用 train()

### 4.4.2. Evaluate 性能评估
（1）**加载评估指标（如 accuracy）**，定义 compute_metrics 函数（将 Logits 转预测结果，计算指标）

（2）**集成到 Trainer**：实例化时传入 compute_metrics，训练中自动评估验证集性能