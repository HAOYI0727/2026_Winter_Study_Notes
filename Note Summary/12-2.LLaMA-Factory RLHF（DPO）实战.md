# LLaMA-Factory RLHF（DPO）实战

<p align="right">2026.02.21</p>

## 1. 核心背景

1. **DPO 算法优势**：相比传统PPO，**DPO无需训练独立奖励模型、省去复杂强化学习采样**，直接在偏好数据上优化策略，更稳定高效。
2. **实战目标**：基于**LLaMA-Factory**框架，使用Qwen2.5-0.5B-Instruct模型，在消费级显卡/CPU上完成DPO对齐流程。

## 2. LLaMA-Factory 核心特点

|特点|具体说明|
|---|---|
|模型支持|覆盖LLaMA、Qwen、Baichuan、ChatGLM等主流开源模型|
|全流程覆盖|支持**预训练、SFT、RLHF（奖励模型/PPO/DPO/KTO/ORPO等）** 全阶段|
|高效微调|内置**LoRA、QLoRA、DoRA、GaLore等PEFT方法**，降低显存需求|
|交互友好|提供LLaMA-Board Web UI，可视化配置参数、监控训练、测试对话|
|硬件适配|支持DeepSpeed、FlashAttention加速，4/8-bit量化训练，适配消费级硬件|

## 3. 环境搭建步骤

1. **克隆仓库并进入**：`git clone https://github.com/hiyouga/LLaMA-Factory.git && cd LLaMA-Factory`
2. **激活环境**：`conda activate peft`
3. **安装依赖**：`pip install -e .[metrics]`
4. **验证安装**：`llamafactory-cli version`（输出版本信息则成功）
5. **启动Web UI**：`llamafactory-cli webui`，访问`http://localhost:7860`进入界面

## 4. DPO 训练准备

### 4.1. 模型配置
- 选择模型：Qwen2.5-0.5B-Instruct（指令微调版，可直接用于DPO）
- 下载源：默认Hugging Face，网络问题可切换ModelScope/OpenMind

### 4.2. 微调参数配置

|参数类别|关键设置|说明|
|---|---|---|
|微调方法|**LoRA**（默认）|基础微调方式，低显存友好|
|量化配置|**8/4-bit**（可选）|显存不足时启用QLoRA，显存充足建议不量化|
|对话模板|**qwen**（Qwen系列专用）|训练/推理需使用相同模板，Base模型可选alpaca/vicuna等|
|其他|**RoPE scaling**（上下文扩展）、**Booster**（auto自动选加速后端）|按需求配置|

### 4.3. 数据集配置
- 阶段选择：Stage下拉菜单选`DPO`
- 数据集选择：使用内置`dpo_zh_demo`（含chosen/rejected偏好数据对）
- 验证：点击`Preview dataset`预览数据格式，确认无误

## 5. 启动 DPO 训练

### 5.1. 核心训练参数

|参数|推荐值|说明|
|---|---|---|
|$Learning \ rate$|1e-4|**学习率**|
|$Epochs$|3.0|**训练轮数**|
|$Compute \ type$|fp16（bf16显卡适配可选）|**计算精度**，fp16兼容性好|
|$Cutoff \ length$|2048|**文本截断长度**|
|$Batch \ size$|1|**单批次样本数**，取决于显存|
|$Gradient \ accumulation$|16|**梯度累积**，建议Batch×Gradient≈16/32|
|$LoRA \ rank$|8|**LoRA秩**，决定微调参数量|
|$LoRA \ alpha$|16|**缩放系数**，默认是rank的2倍|

### 5.2. 训练操作
- 预览命令：设置输出目录后，点击`Preview command`确认训练指令
- 启动训练：点击`Start`，训练完成后适配器权重保存至`saves/Qwen2.5-0.5B-Instruct/lora/Qwen2.5-0.5B-dpo-demo`
- 监控：在Train面板查看Loss曲线，观察模型收敛情况

## 6. 效果测试与对比

### 6.1. 测试流程
- 原始模型测试：不加载Adapter，直接加载Qwen2.5-0.5B-Instruct，输入测试问题验证表现
- DPO模型测试：选择训练好的Adapter路径，加载混合权重后重复测试

### 6.2. 核心对比结果
|模型类型|回答特点|
|---|---|
|原始模型|简短武断、忽略指令要求、倾向表面关联猜测答案|
|DPO模型|严谨、遵循指令展示推理过程、明确指出信息局限性|

### 6.3. 结论
DPO训练未仅优化“答案正确性”，更对齐人类偏好的交互风格（如严谨、有推理过程）。