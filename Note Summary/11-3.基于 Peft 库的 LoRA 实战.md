# 基于 Peft 库的 LoRA 实战

<p align="right">2026.02.14</p>

## 1. 核心背景

（1）**peft库**：Hugging Face旗下的参数高效微调（PEFT）工具库，非替代transformers等基础模型库，而是作为插件/增强模块，以统一简洁接口封装LoRA、Prefix Tuning等PEFT方法，实现大模型高效微调，节省计算/存储资源，提升模型管理与部署灵活性。

（2）**类比**：
- **基础预训练模型**：如同游戏主角，具备基础能力；
- **peft库**：如同主角的“法术神通库”，包含各类PEFT方法；
- **PeftConfig**：如同“法术搭配方案”，定义微调策略；
- **get_peft_model函数**：如同“临阵变身”，将PEFT方法应用到基础模型，生成PeftModel。

## 2. peft库核心组件

### 2.1. PeftConfig（声明式配置）

（1）**基类通用参数**：
- $peft\_type$：指定**PEFT插件类型**（如PeftType.LORA）；
- $task\_type$：指定**下游任务类型**（如TaskType.CAUSAL_LM用于自回归语言模型）。

（2）**LoraConfig（LoRA专属配置）**：
- $r$：**LoRA的秩**，控制新增参数量和模型适应能力；
- $lora\_alpha$：**缩放因子**，调整低秩矩阵与原始权重矩阵合并尺度；
- $target\_modules$：指定**应用LoRA的模型模块**（如["q_proj", "v_proj"]）；
- $lora\_dropout$：LoRA层的**Dropout比例**，防止过拟合；
- $bias$：**偏置参数**训练方式（none/all/lora_only）。

### 2.2. PeftModel（动态注入生成）

（1）**核心函数**：$get\_peft\_model$接收**基础模型和PeftConfig**，解析配置后遍历模型结构，替换/封装目标层为注入LoRA的模块，返回PeftModel。

（2）**关键方法**：$print\_trainable\_parameters()$，计算并打印可训练参数数量及占比，直观体现PEFT的资源优势。

## 3. LoRA微调实战流程（以Pythia-2.8b模型为例）

### 3.1. 环境与模型准备

1. **依赖加载**：导入transformers、peft、torch等库；
2. **模型量化加载**：使用BitsAndBytesConfig定义8-bit量化配置，加载模型时指定quantization_config、dtype=torch.float16、device_map="auto"，优化显存占用；
3. **分词器处理**：加载对应分词器，若无pad_token则手动设置为eos_token（适配批量训练）；
4. **模型预处理**：调用prepare_model_for_kbit_training函数，完成类型转换、启用梯度检查点、输出嵌入层预处理、输入梯度处理等，适配k-bit量化模型的LoRA微调。

```Python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch

model_id = "EleutherAI/pythia-2.8b-deduped"

# --- 使用 BitsAndBytesConfig 定义 8-bit 量化配置 ---
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

# 加载模型，并将量化配置传给 `quantization_config` 参数
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=bnb_config,
    dtype=torch.float16,
    device_map="auto",
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
# Pythia模型的tokenizer默认没有pad_token，我们将其设置为eos_token
tokenizer.pad_token = tokenizer.eos_token

from peft import prepare_model_for_kbit_training

# 对量化后的模型进行预处理
model = prepare_model_for_kbit_training(model)
```

### 3.2. 定义LoRA配置并创建PeftModel

```Python

from peft import LoraConfig, get_peft_model
# 定义配置
config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["query_key_value", "dense"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
# 生成PeftModel
peft_model = get_peft_model(model, config)
peft_model.print_trainable_parameters() # 示例输出：可训练参数仅占0.28%
```

### 3.3. 数据处理

1. **加载数据集**：使用datasets库加载目标数据集（如Abirate/english_quotes名言数据集）；
2. **数据分词**：定义分词函数，提取目标字段（如quote），调用tokenizer编码生成input_ids和attention_mask；
3. **批量处理**：通过dataset.map()批量应用分词函数，完成数据集预处理。

```Python
from datasets import load_dataset

# 加载数据集
quotes_dataset = load_dataset("Abirate/english_quotes")

# 查看数据集示例
quotes_dataset['train'][0]

# 定义分词函数
def tokenize_quotes(batch):
    # 只对 "quote" 列进行分词
    return tokenizer(batch["quote"], truncation=True)

# 对整个数据集进行分词处理
tokenized_quotes = quotes_dataset.map(tokenize_quotes, batched=True)

tokenized_quotes['train'][0]
```

### 3.4. 模型训练

1. **训练参数配置（TrainingArguments）**：
- $per\_device\_train\_batch\_size/gradient\_accumulation\_steps$：控制有效批量大小；
- $warmup\_steps$：学习率预热步数；
- $max\_steps/num\_train\_epochs$：控制训练总长度；
- $learning\_rate$：参数更新幅度；
- $fp16$：**启用混合精度训练**，减少显存占用、加速训练；
- 其他：$logging\_steps$（日志打印步数）、$output\_dir$（模型保存路径）。

2. **数据整理器**：使用$DataCollatorForLanguageModeling$处理批量数据；

3. **训练器实例化与训练**：

```Python
from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling

# 推荐操作：关闭缓存可提高训练效率
peft_model.config.use_cache = False

# 定义训练参数
train_args = TrainingArguments(
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    warmup_steps=100,
    max_steps=200,
    learning_rate=2e-4,
    fp16=True, # 启用混合精度训练
    logging_steps=1,
    output_dir="outputs",
)

# 数据整理器，用于处理批量数据
quote_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# 实例化 Trainer
trainer = Trainer(
    model=peft_model,
    train_dataset=tokenized_quotes["train"],
    args=train_args,
    data_collator=quote_collator,
)

# 开始训练
trainer.train()
```

### 3.5 模型保存与推理

1. **模型保存**：调用peft_model.save_pretrained()，仅保存LoRA适配器权重（几十MB）；
2. **权重合并（可选）**：peft_model.merge_and_unload()将LoRA矩阵合并回原始权重，生成标准transformers模型，消除推理延迟；
3. **推理验证**：
    - 模型设为评估模式，配置pad_token_id；
    - 输入prompt分词后，调用generate方法生成文本（关键参数：**max_length、do_sample、temperature、top_p、top_k、repetition_penalty**等）；
    - 解码输出并验证效果，示例中模型成功补全并续写名言，体现微调有效性。

```Python
# 将模型设置为评估模式
peft_model.eval()

# 设置 pad_token_id 到模型配置中
peft_model.config.pad_token_id = tokenizer.pad_token_id

prompt = "Be yourself; everyone"

# 对输入进行分词，并获取 attention_mask
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs["input_ids"].to(peft_model.device)
attention_mask = inputs["attention_mask"].to(peft_model.device)

# 生成文本
with torch.no_grad():
    # 使用 autocast 提高混合精度推理的效率
    with torch.amp.autocast('cuda'):
        outputs = peft_model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=50,
            do_sample=True,
            temperature=0.6,
            top_p=0.95,
            top_k=40,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.pad_token_id
        )

# 解码并打印结果
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
decoded_output
```

> 注意点
> 1. **target_modules选择**：需根据模型结构确定（如Pythia模型为query_key_value、dense等）；
> 2. **训练参数权衡**：max_steps适合快速验证，num_train_epochs适合正式训练；建议划分验证集监控过拟合；
> 3. **推理随机性**：采样策略（do_sample=True）是输出多样性的主要来源，固定种子仍可能因底层计算有微小差异，属正常现象。