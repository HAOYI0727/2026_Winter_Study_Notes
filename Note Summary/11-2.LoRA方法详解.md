# LoRA方法详解

<p align="right">2026.02.12</p>

## 1. 核心背景
**LoRA（Low-Rank Adaptation）** 是参数高效微调（PEFT）的主流方法，解决全量微调成本高、Adapter有推理延迟、Prompt Tuning优化难/占输入长度的问题，直击模型权重矩阵，通过低秩近似大幅减少可训练参数。

## 2. LoRA核心原理

### 2.1. 低秩近似思想

大模型微调时的权重更新矩阵$ΔW$具有**低内在秩**，可用两个小矩阵乘积近似：$ΔW = B·A（W₀∈ℝ(d×k)，B∈ℝ(d×r)，A∈ℝ(r×k)，r≪min(d,k)）$。

（1）**结构**：原始权重$W₀$冻结（主路），**并行旁路为A（降维）+ B（升维）**，最终输出$h = W₀·x + s·(B·A)·x$（s为缩放因子，通常$α/r$）。

（2）**初始化**：**A高斯随机初始化，B全零初始化**（训练初期旁路输出为0，保证稳定性）。

（3）**参数效率**：可训练参数从$d×k$降至$d×r + r×k$（r取8/16/64，仅为全量微调的千分之一/万分之一）。

### 2.2. 核心优势

1. **高参数/存储效率**：仅存小矩阵A/B，checkpoint体积缩10000倍，训练显存省2/3，速度提25%
2. **零推理延迟**：训练后可将ΔW合并回W₀，网络结构与原模型一致
3. **效果优+不占输入长度**：直接修改权重，效果接近全量微调；无virtual token，适配长文本任务
4. **可组合性**：可与Prefix-Tuning等PEFT方法结合

### 2.3. 关键实践

（1）**适用权重**：优先对**自注意力模块$（W_q/W_k/W_v/W_o）$**应用，尤其$W_q+W_v$组合效果最优，冻结其余模块。

（2）**秩r选择**：小r（1/2/4/8）即可达优效，盲目增大r会增参数量甚至降性能。

（3）**生效逻辑**：$ΔW$学习预训练中未充分强调、但对下游任务关键的“隐藏特征”，精准补全模型能力。

## 3. AdaLoRA（自适应LoRA）

### 3.1. 解决问题

原始LoRA秩r固定、仅聚焦注意力模块，未考虑不同任务/层/矩阵的“可塑性”差异。

### 3.2. 核心创新

（1）**基于SVD的参数化**：$ΔW = PΛQ$（P：左奇异向量，Λ：对角奇异值矩阵，Q：右奇异向量），无需真实SVD分解，训练时$P/Λ/Q$可训，且加**正交正则项**保证稳定性；Λ的奇异值控制更新幅度，P/Q控制更新方向。

（2）**重要性评分与动态预算分配**：
- 单参数重要性：$s(w) = 平滑后敏感度（|w·∇_wℒ|的EMA） × 平滑后不确定性（敏感度波动的EMA）$。
- 三元组$（P_{k,i}, λ_{k,i}, Q_{k,i}）$重要性：聚合内部所有参数的$s(w)$。
- 按分数排序裁剪低重要性三元组，实现参数动态分配。

（3）**全局预算调度器**:**热身**：高初始预算，探索参数重要性；**裁剪**：按三次方曲线降预算至目标值；**微调**：固定预算，收敛模型。

### 3.3. 优化效果

自动为**FFN模块、模型高层**分配更高秩，相同参数预算下性能优于原始LoRA；仅替换SVD参数化形式也能提效。

## 4. QLoRA（量化LoRA）

### 4.1. 核心目标

进一步**降低显存开销**，实现大模型（如65B）单卡（48GB）微调，性能接近16-bit全量微调。

### 4.2. 关键技术

（1）**$4-bit \ NormalFloat \ (NF4)$**：针对权重正态分布设计的最优量化类型，基于**分位数量化**（每个量化桶包含等数量值），先归一化权重到[-1,1]，再映射到16个NF4分位数点，精度远优于传统4-bit量化；

（2）**双量化（DQ）**：**对量化常数（32-bit）再做8-bit量化**，将额外开销从0.5bit/参数降至0.127bit/参数

（3）**分页优化器**：借鉴虚拟内存思想，显存不足时**将优化器状态分页到CPU**，避免OOM

### 4.3. 工作流程

（1）**存储**：加载$16-bit$模型→量化为$4-bit \ NF4$（双量化压缩常数），**冻结基座模型**；

（2）**计算**：前向时动态反量化基座权重至$16-bit \ BF16$，与$16-bit \ LoRA$适配器（L1/L2）并行计算，计算后**释放临时高精度权重**；

（3）**更新**：梯度仅反向传播到$16-bit \ LoRA$适配器，**分页优化器管控显存峰值**。


> 总结
> |方法|核心改进|显存/参数效率|推理延迟|
> |---|---|---|---|
> |Adapter|串行激活值瓶颈结构|较高|增加|
> |LoRA|并行权重低秩旁路|高|无|
> |AdaLoRA|自适应秩分配+SVD参数化|更高|无|
> |QLoRA|4-bit量化+双量化+分页优化器|极高|无|