# 第二章 文本表示与词向量

<p align="right">2026.01.29</p>

# 1. 初级分词技术

## 1.1. 分词核心概念

（1）定义：**分词（Tokenization）是将连续文本切分为具有独立语义的基本单元（词/词元）的过程**。
- 英文等语言有天然空格分隔，分词简单；
- 中文等无边界语言，需算法切分（如“给阿姨倒一杯卡布奇诺”→["给", "阿姨", "倒", "一杯", "卡布奇诺"]）。

（2）重要性
- 传统NLP中是后续任务（信息检索、机器翻译等）的基础，分词错误会引发级联效应（如“南京市长江大桥”错分为["南京", "市长", "江大桥"]导致搜索失效）；
- 现代NLP通过灵活切分策略缓解该问题。

## 1.2. jieba：传统分词工具

### 1.2.1. 安装：
```Bash
pip install jieba
pip show jieba  # 验证安装
```

### 1.2.2. 核心原理：基于规则与词典

（1）核心流程：依赖前缀词典（**Trie树**）构建有向无环图（**DAG**），通过**动态规划**找概率最大的分词路径。
- **路径概率**： $ P(w_1,w_2,...,w_n) \approx \prod_{i=1}^n P(w_i) $ ， $ P(w_i) = \frac{词w_i词频}{总词频} $ ；
- **优化**：用对数概率将累乘转累加（避免浮点数下溢），目标为  $ \underset{w}{\arg\max} \sum_{i=1}^{n} \log P(w_i) $ ；
- **动态规划：从句子末尾向前递推，记录每个位置最优切分路径**。

（2）精确模式实践
- 基础用法：切分词典中已有词，适合常规文本分析。
    ```Python
    
    import jieba
    text = "我在梦里收到清华大学录取通知书"
    seg_list = jieba.lcut(text, cut_all=False)  # 精确模式
    print(seg_list)  # ['我', '在', '梦里', '收到', '清华大学', '录取', '通知书']
    ```

- 自定义词典解决OOV（未登录词）问题：
    - 词典格式：`词语 [词频] [词性]`（词频/词性可选）；
    - 效果：未加载时“九头虫让奔波儿灞把唐僧师徒除掉”被错切，加载后可正确识别“九头虫”“奔波儿灞”。
    ```Python
    
    # 加载自定义词典前后对比
    text = "九头虫让奔波儿灞把唐僧师徒除掉"
    print(f"精准模式: {jieba.lcut(text, cut_all=False)}")
    jieba.load_userdict("./user_dict.txt") 
    print(f"加载词典后: {jieba.lcut(text, cut_all=False)}")
    ```

（3）精确模式工作流程
1. **文本预处理**：切分汉字区块与非汉字部分，仅处理汉字；
2. **构建DAG**：扫描句子，找出每个字可组成的所有词，生成有向无环图；
3. **计算最优路径**：动态规划从后向前计算每个位置最优路径的log概率和；
4. **重建结果**：根据路由表回溯，输出最终分词结果。

### 1.2.3. HMM（隐马尔可夫模型）

（1）核心思想：**将分词转为序列标注问题**（B/M/E/S标签体系：B-词开始，M-词中间，E-词结束，S-单字成词），学习字与标签、标签与标签的概率关系。

（2）jieba中的应用：识别未登录词
- 混合策略：动态规划生成的单字存入缓冲区，遇到多字词/句尾时，调用HMM对缓冲区二次分词；
- 效果：开启HMM可识别“直聘”等未登录词，关闭则拆分为单字。
    ```Python
    
    text = "我在Boss直聘找工作"
    seg_list_hmm = jieba.lcut(text, HMM=True)  # 开启HMM（默认）
    seg_list_no_hmm = jieba.lcut(text, HMM=False)  # 关闭HMM
    print(f"HMM开启: {seg_list_hmm}")  # ['我', '在', 'Boss', '直聘', '找', '工作']
    print(f"HMM关闭: {seg_list_no_hmm}")  # ['我', '在', 'Boss', '直', '聘', '找', '工作']
    ```

（3）词性标注
- 模块：`jieba.posseg`，**结合词典+HMM标注词性**；
- 自定义词典可干预词性/分词结果：调整词频可强制拆分（如“九头虫”拆为“九/头/虫”），指定词性可修正标注（如“奔波儿灞”标注为nr）。
    ```Python
    
    import jieba.posseg as pseg
    text = "九头虫让奔波儿灞把唐僧师徒除掉"
    jieba.load_userdict("./user_pos_dict.txt")
    words = pseg.lcut(text, HMM=False)
    print(f"加载词性词典后: {words}")
    ```
- 常见词性标签：n（名词）、v（动词）、nr（人名）、x（非语素字）等。

## 1.3. 现代分词策略：从“分词”到“分块”

（1）字粒度（代表：BERT）
- 策略：将每个汉字视为独立Token；
- 优点：解决OOV问题，字表覆盖全；
- 缺点：丢失词汇语义，序列长度增加，计算负担大。

（2）子词粒度（代表：GPT，主流算法BPE）
- 策略：迭代合并语料中高频相邻字符对，生成子词；
- 优点：平衡词表大小与语义表达，高频词完整保留，低频/新词拆分为有意义单元，解决OOV；
- 缺点：可解释性差，依赖训练语料，领域外文本易过度切分。

# 2. 词向量表示

## 2.1. 核心背景
（1）**矛盾**：计算机只能处理数值形式（特征向量/矩阵），但文本是字符串，需弥合自然语言（符号）与数学模型（向量空间）的鸿沟。

（2）**目标**：将词元序列转为有意义的数字，理想状态下向量需蕴含语义信息（如“国王”与“女王”向量距离更近）。

（3）**词嵌入（Word Embedding）**：**神经网络学习得到的稠密、高效的词向量表示**，是词向量的重要类型。

## 2.2. 离散表示（传统机器学习）

### 2.2.1. 独热编码（One-Hot Encoding）
- **层级**：**词元**级别
- **原理**：构建唯一词典→分配整数索引→生成等长向量（仅对应索引位为1，其余为0）。
- **优点**：实现简单，能清晰区分词语。
- **缺点**：**维度灾难**：词典规模大时向量维度极高，数据稀疏；**语义鸿沟**：任意两不同词向量正交（点积为0），无法体现语义相似性。

### 2.2.2. 词袋模型（Bag-of-Words, BoW）
- **层级**：**文档**级别
- **核心思想**：忽略词序/语法，将文本视为“词袋”，用词的统计量表示文档（本质是文档内所有词的独热向量求和）。
- **关键计算**：**余弦相似度**（衡量文档相似性，值越接近1越相似）。
- **统计方式**：频数（词出现次数）、频率（频数/总词数）、二进制（仅标记是否出现）。
- **优点**：实现简单，文本分类任务表现好；
- **缺点**：丢失词序（无法区分“我爱你”和“你爱我”）、未考虑词重要性（停用词干扰）。

### 2.2.3. TF-IDF
- **核心理念**：**词的重要性 = 文档内出现频次（TF）× 语料库中稀有程度（IDF）（当前文档高频+语料库低频→权重高）**。
- **组成**：
  - TF：原始频数/归一化频率（消除文档长度偏差）；
  - IDF：$\log \frac{|D|}{1 + |\{d \in D : t \in d\}|}$（平滑版，避免除零）；
  - 最终权重：TF × IDF。
- **应用**：关键词提取、文本相似度计算、传统搜索引擎相关性衡量。

### 2.2.4. N-gram模型
- **核心**：弥补词序丢失，基于**马尔可夫假设**（词的概率仅依赖前N-1个词），是大模型雏形（预测下一个词）。
- **类型**：Unigram（1-gram，无词序）、Bigram（2-gram，依赖前1词）、Trigram（3-gram，依赖前2词）。
- **优点**：捕捉词序；
- **缺点**：指数级维度爆炸、数据稀疏（需平滑技术/类模型缓解）。

## 2.3. 序号化表示（深度学习时代）

### 2.3.1. 核心逻辑
- 仅做最少预处理：将文本转为整数ID序列，把“学习词的含义/重要性”交给模型自身。
- 解决问题：规避离散表示的维度灾难、语义鸿沟等问题，适配深度学习批处理需求。

### 2.3.2. 序号化过程
1. **构建词典**：字级别（BERT）/子词级别（GPT）（而非词级别）；
2. **增加特殊词元**：[PAD]（0，填充至统一长度）、[UNK]（1，表示未登录词），可选[CLS]（分类）、[SEP]（分隔）；
3. **ID映射**：词元→词典中整数ID（常用预训练模型的官方词典）。

### 2.3.3. 实例
- **词典映射→短序列用[PAD]填充→生成统一维度的整数矩阵（模型最终输入）**；
- 关键补充：序号化ID无数学意义，需通过嵌入层转为低维稠密浮点数向量（词向量），映射关系由模型训练学习。

# 3. 从主题模型到Word2Vec

## 3.1. 核心目标

（1）传统词表示的缺陷：**哑编码/序号化无法表达词间语义关系**（如正交、ID无语义关联），无法度量“国王-女王”比“国王-苹果”语义更近。

（2）理想词向量的要求：
- **语义蕴含**：向量距离能度量语义相似度（基于分布式假设：上下文相似的词，语义相近）。
- **低维稠密**：摆脱维度灾难，维度可控（超参数），向量为有意义的浮点数（非稀疏）。

（3）技术路径
| 路径 | 核心思路 | 代表方法 |
|------|----------|----------|
| **全局文档统计** | 基于全局词共现统计捕捉语义 | 主题模型（LSA/SVD） |
| **局部上下文预测** | 基于局部上下文预测任务学习语义 | Word2Vec |

## 3.2. 主题模型：基于全局统计的词向量

### 3.2.1. 核心假设

一篇文档由多个主题按比例混合而成，一个主题由多个词语按概率组成；**词的向量用“词-主题关联强度”表示**。

### 3.2.2. 关键步骤（SVD矩阵分解）
1. **构建词-文档矩阵**：行=词，列=文档，值=TF-IDF（巨大、稀疏）。
2. **SVD分解**：$X_{m×n} ≈ W_{topic(m×k)} × H_{topic(k×n)}$
   - $k$：潜在主题数（远小于$m/n$）；
   - $W_{topic}$：词-主题矩阵（每行=k维词向量）；
   - $H_{topic}$：文档-主题矩阵（每列=文档的主题分布）。
3. **获取词向量**：取$W_{topic}$的每行作为词向量（降维、稠密、含语义）。

### 3.2.3. 局限性
- 计算代价高（大规模语料SVD分解开销大）；
- 忽略局部上下文和词序，无法捕捉精细语义；
- 难以与深度学习端到端联合训练。

## 3.3. Word2Vec：基于局部上下文预测的词向量

### 3.3.1. 核心思想
基于**分布式假设**（词的含义由上下文决定），通过“**伪任务**”（**上下文↔中心词预测**）训练，最终获取词向量查询表（而非保留模型）。

### 3.3.2. 本质
- **浅层神经网络**（移除传统NNLM的非线性隐藏层，高效）；
- 最终目标：得到高质量词向量查询表$W_{in}$（每行=词的稠密向量）；
- 实现手段：以“**上下文预测中心词/中心词预测上下文**”为伪任务，将$W_{in}$作为模型参数训练。

### 3.3.3. 词向量矩阵$W_{in}$的工作原理
- 输入：单词ID → 哑编码（One-Hot）；
- 计算：One-Hot向量 × $W_{in}$（等效直接查表取对应行）；
- 特性：$W_{in}$随机初始化，通过训练优化（如PyTorch的nn.Embedding层）。

### 3.3.4. 两种核心模型
| 模型 | 任务 | 核心流程 | 特点 |
|------|------|----------|------|
| **CBOW**（连续词袋） | **上下文→中心词** | 1. 上下文词ID→词向量；2. 词向量聚合（平均/求和）得上下文向量；3. 上下文向量×$W_{out}$得得分；4. Softmax+交叉熵损失优化$W_{in}$/$W_{out}$ | 训练快，适合高频词 |
| **Skip-gram** | **中心词→上下文** | 1. 中心词ID→词向量；2. 词向量×$W_{out}$得得分；3. 得分复用，与多个上下文词计算损失并求和优化 | 学习精细语义，适合低频词/大数据集，训练慢 |

### 3.3.5. 语义捕捉原理（滑动窗口）
滑动窗口生成大量重叠样本，模型为最小化损失，会让“上下文相似的词”的向量在空间中靠近（本质是最大化向量点积，减小夹角）；可通过**Hierarchical Softmax/负采样**加速训练。

## 3.4. Word2Vec的局限性
（1）**上下文无关**：一个词仅对应一个固定向量，无法解决一词多义（如“小米”=粮食/公司，向量相同）；

（2）**静态本质**：训练后词向量查询表固定，无动态上下文分析。

# 4. Gensim词向量实战

## 4.1. Gensim核心认知
### 4.1.1. 简介
Gensim是高效处理非结构化文本的Python库，内置Word2Vec、TF-IDF、LSA、LDA等主流词向量/主题模型算法，核心能力是实现文本到向量的转换。

### 4.1.2. 核心概念
| 概念 | 定义|
|-----|-----|
| **语料库** | 训练数据集，分词后为`list[list[str]]`；BoW语料库是稀疏向量可迭代对象 |
| **词典** | 词语→唯一整数ID的映射表，BoW模型前需先构建  |
| **向量** | 文档的数学表示，如BoW向量为`[(token_id, frequency), ...]` |
| **稀疏向量** | 仅记录非零项索引和值，节省内存（如`[2,1,0,...0]`→`[(0,2),(1,1)]`） |
| **模型** | 实现向量转换的算法（如TfidfModel将BoW向量转TF-IDF权重向量） |

### 4.1.3. 安装与内置算法
（1）安装：`pip install gensim`

（2）内置算法分类：
  - 基于BoW：TF-IDF、LSA、LDA、NMF
  - 神经网络词向量：Word2Vec、FastText、Doc2Vec

## 4.2. Gensim核心工作流（分两类）
### 4.2.1. 基于BoW的模型（TF-IDF/LSA/LDA/NMF）
1. **准备语料**：分词后整理为`list[list[str]]`
2. **创建词典**：映射词语到整数ID
3. **词袋化**：将文档转为稀疏BoW向量（`[(token_id, frequency), ...]`）→ 作为模型输入

```Python
import jieba
from gensim import corpora

# Step 1: 准备分词后的语料 (新闻标题)
raw_headlines = [
    "央行降息，刺激股市反弹",
    "球队赢得总决赛冠军，球员表现出色"
]
tokenized_headlines = [jieba.lcut(doc) for doc in raw_headlines]
print(f"分词后语料: {tokenized_headlines}")

# Step 2: 创建词典
dictionary = corpora.Dictionary(tokenized_headlines)
print(f"词典: {dictionary.token2id}")

# Step 3: 转换为BoW向量语料库
corpus_bow = [dictionary.doc2bow(doc) for doc in tokenized_headlines]
print(f"BoW语料库: {corpus_bow}")
```

### 4.2.2. 神经网络词向量模型（Word2Vec/FastText/Doc2Vec）
**无需词袋化**，直接以分词后的`list[list[str]]`为输入。

## 4.3. 核心模型实战
### 4.3.1. TF-IDF：计算关键词权重
（1）作用：**衡量词在文档中的重要性**，高频且少跨文档的词权重高（如标点/停用词权重低）

（2）核心步骤：
  1. **分词构建语料→创建词典→生成BoW语料库**
  2. 训练`TfidfModel`，将BoW向量转为TF-IDF权重向量
  3. 新文本需先转BoW，再用训练好的模型生成TF-IDF向量（OOV词会被忽略）

```Python
import jieba
from gensim import corpora, models

# 1. 准备语料 (新闻标题，包含财经和体育两个明显主题)
headlines = [
    "央行降息，刺激股市反弹",
    "球队赢得总决赛冠军，球员表现出色",
    "国家队公布最新一期足球集训名单",
    "A股市场持续震荡，投资者需谨慎",
    "篮球巨星刷新历史得分记录",
    "理财产品收益率创下新高"
]
tokenized_headlines = [jieba.lcut(title) for title in headlines]

# 2. 创建词典和BoW语料库
dictionary = corpora.Dictionary(tokenized_headlines)
corpus_bow = [dictionary.doc2bow(doc) for doc in tokenized_headlines]

# 3. 训练TF-IDF模型
tfidf_model = models.TfidfModel(corpus_bow)

# 4. 将BoW语料库转换为TF-IDF向量表示
corpus_tfidf = tfidf_model[corpus_bow]

# 辅助函数：把 (token_id, weight) 转成 (token, weight)，并按权重降序展示
def tfidf_with_words(tfidf_vec, id2word):
    pairs = [(id2word[token_id], weight) for token_id, weight in tfidf_vec]
    return sorted(pairs, key=lambda x: x[1], reverse=True)

# 打印第一篇标题的TF-IDF向量
first_tfidf = list(corpus_tfidf)[0]
print("第一篇标题的TF-IDF向量:")
print(first_tfidf)
print("第一篇标题的TF-IDF向量(带词语):")
print(tfidf_with_words(first_tfidf, dictionary))

# 5. 对新标题应用模型
new_headline = "股市大涨，牛市来了"
new_headline_bow = dictionary.doc2bow(list(jieba.cut(new_headline)))
new_headline_tfidf = tfidf_model[new_headline_bow]
print("\n新标题的TF-IDF向量:")
print(new_headline_tfidf)
```

### 4.3.2. LDA：挖掘文档主题
（1）作用：**无监督发现文档隐藏主题**，输出文档的主题概率分布

（2）核心步骤：
  1. 同TF-IDF的前两步（语料→词典→BoW）
  2. 训练`LdaModel`（指定主题数`num_topics`）
  3. 查看主题关键词（`print_topics()`），推断新文档主题分布

（3）注意：新文本若与词典无重叠词，主题分布接近均匀

```Python
from gensim import corpora, models

# 1. 准备语料
headlines = [
    "央行降息，刺激股市反弹",
    "球队赢得总决赛冠军，球员表现出色",
    "国家队公布最新一期足球集训名单",
    "A股市场持续震荡，投资者需谨慎",
    "篮球巨星刷新历史得分记录",
    "理财产品收益率创下新高"
]
tokenized_headlines = [jieba.lcut(title) for title in headlines]

# 2. 创建词典和BoW语料库
dictionary = corpora.Dictionary(tokenized_headlines)
corpus_bow = [dictionary.doc2bow(doc) for doc in tokenized_headlines]

# 3. 训练LDA模型 (假设需要发现2个主题)
lda_model = models.LdaModel(corpus=corpus_bow, id2word=dictionary, num_topics=2, random_state=100)

# 4. 查看模型发现的主题
print("模型发现的2个主题及其关键词:")
for topic in lda_model.print_topics():
    print(topic)

# 5. 推断新文档的主题分布
new_headline = "巨星詹姆斯获得常规赛MVP"
new_headline_bow = dictionary.doc2bow(jieba.lcut(new_headline))
topic_distribution = lda_model[new_headline_bow]
print(f"\n新标题 '{new_headline}' 的主题分布:")
print(topic_distribution)
```

### 4.3.3. Word2Vec：学习语义词向量
（1）作用：从上下文学习稠密的词语语义向量，核心是得到`model.wv`（词向量查询表）

（2）核心步骤：
  1. 准备分词后语料（`list[list[str]]`）
  2. 训练`Word2Vec`，关键参数：
     - `vector_size`：词向量维度（50-300）
     - `window`：上下文窗口大小
     - `min_count`：最小词频（过滤噪音）
     - `sg`：算法选择（0=CBOW，1=Skip-gram）
  3. 词向量应用：找相似词、计算余弦相似度、获取词向量
  4. 模型持久化：推荐保存`model.wv`（无训练冗余信息，节省存储/内存）

```Python
# 模型训练与核心参数
from gensim.models import Word2Vec

# 1. 准备语料
headlines = [
    # 财经
    "央行降息，刺激股市反弹",
    "A股市场持续震荡，投资者需谨慎",
    "理财产品收益率创下新高",
    "证监会发布新规，规范市场交易",
    "创业板指数上涨，科技股领涨大盘",
    "房价调控政策出台，房地产市场降温",
    "全球股市动荡，影响资本市场信心",
    "分析师认为，当前股市风险与机遇并存，市场情绪复杂",

    # 体育
    "球队赢得总决赛冠军，球员表现出色",
    "国家队公布最新一期足球集训名单",
    "篮球巨星刷新历史得分记录",
    "奥运会开幕，中国代表团旗手确定",
    "马拉松比赛圆满结束，选手创造佳绩",
    "电子竞技联赛吸引大量年轻观众",
    "这支球队的每位球员都表现出色",
    "球员转会市场活跃，多支球队积极引援"
]
tokenized_headlines = [jieba.lcut(title) for title in headlines]


# 2. 训练Word2Vec模型
model = Word2Vec(tokenized_headlines, vector_size=50, window=3, min_count=1, sg=1)

# 使用词向量
# 1. 寻找最相似的词
# 在小语料上，结果可能不完美，但能体现出模型学习到了主题内的关联
similar_to_market = model.wv.most_similar('股市')
print(f"与 '股市' 最相似的词: {similar_to_market}")

# 2. 计算两个词的余弦相似度
similarity = model.wv.similarity('球队', '球员')
print(f"\n'球队' 和 '球员' 的相似度: {similarity:.4f}")

# 3. 获取一个词的向量
market_vector = model.wv['市场']
print(f"\n'市场' 的向量维度: {market_vector.shape}")

# 模型的持久化
from gensim.models import KeyedVectors

# 保存词向量到文件
model.wv.save("news_vectors.kv")

# 从文件加载词向量
loaded_wv = KeyedVectors.load("news_vectors.kv")

# 加载后可以执行同样的操作
print(f"\n加载后，'球队' 和 '球员' 的相似度: {loaded_wv.similarity('球队', '球员'):.4f}")
```

## 4.4. 注意事项
1. 标点/停用词会影响权重/主题，建议构建词典前移除
2. 新文本中词典外的词（OOV）会被模型忽略
3. Word2Vec小语料训练的相似度数值低且不稳定，仅作演示
4. 仅需使用词向量时，保存`KeyedVectors`（`model.wv`）而非完整模型
