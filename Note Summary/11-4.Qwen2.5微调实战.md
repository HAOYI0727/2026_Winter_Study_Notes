# Qwen2.5微调实战

<p align="right">2026.02.15</p>

## 1. 核心背景

### 1.1. 微调前提

参数高效微调（PEFT）需结合模型架构特点，不同模型的LoRA配置（如`target_modules`）不可直接复用，需适配模型结构。

### 1.2. Qwen2.5模型核心特点

|维度|关键信息|
|---|---|
|预训练|18万亿Token高质量数据，强化代码/数学领域，通用能力强|
|架构|**Decoder-only Transformer，含分组查询注意力、SwiGLU激活、旋转位置编码、RMSNorm、QKV偏置**|
|交互层|字节级BPE分词器（词表151643）；内置对话模板，通过 <\|im_start\|> 和 <\|im_end\|> 界定角色和内容的边界|

## 2. Qwen2.5微调策略

（1）**任务类型适配**：`task_type="CAUSAL_LM"`，数据需构造成`输入→输出`的因果序列格式；

（2）**交互规则正确**：严格用官方对话模板格式化数据，调用`apply_chat_template`保证格式无误；

（3）**LoRA目标模块**：优先选择注意力（`q_proj/k_proj/v_proj/o_proj`）和前馈网络（`gate_proj/up_proj/down_proj`）的线性层；

（4）**利用先天优势**：基于Qwen2.5已有的知识/代码/数学基础，微调特定领域任务事半功倍。

## 3. 实战流程

### 3.1. 私有数据集构建（以《黑神话：悟空》为例）

（1）前置评估
- 加载4-bit量化的`Qwen2.5-7B-Instruct`，验证基础模型对目标领域知识的缺失；
- 定义推理函数，通过对话模板格式化输入，测试模型回答效果（存在细节偏差）。

（2）数据生成步骤
|阶段|操作|
|---|---|
|源数据处理|切分Markdown文档为≥100字符的知识片段；|
|基础问答生成|用235B大模型作为“教师模型”，将知识片段转为`(instruction, output)`基础问答对；|
|数据增强|对每个基础问题生成多版本等价问法，提升数据集多样性；|
|输出格式|最终保存为JSONL格式，每条含`instruction`（问题）和`output`（答案）。|

### 3.2. QLoRA微调核心步骤

（1）**环境与组件加载**
- 分词器：加载Qwen2.5分词器，确认`pad_token`/`eos_token`配置；
- 模型量化：通过`BitsAndBytesConfig`配置4-bit量化（NF4类型+双重量化+bfloat16计算），调用`prepare_model_for_kbit_training`做训练预处理；
- LoRA配置：复用`peft`内置的`qwen2`目标模块映射，核心参数`r=16`、`lora_alpha=32`、`lora_dropout=0.05`、`task_type="CAUSAL_LM"`。

（2）**数据格式化**
- 构造仅含问题的prompt和含答案的完整对话，生成`input_ids`；
- 标签（`labels`）仅保留答案部分，问题部分设为`-100`（忽略损失计算）。

（3）训练配置
- 关键参数：`per_device_train_batch_size=1`、`gradient_accumulation_steps=8`、`lr=1e-3`、`bf16=True`（硬件支持时）；
- 训练器：用`Trainer`训练，保存LoRA适配器权重。

### 3.3. 效果评估
- 微调后模型能准确回答目标领域问题（如《黑神话：悟空》道具合成、剧情相关）；
- 注意风险：小数据集易过拟合，需控制训练步数/学习率。

## 4. 技术选型决策框架
|技术方案|优化维度|适用场景|成本/修改程度|
|---|---|---|---|
|提示词工程|上下文优化|任务简单、模型已有相关知识|最低，无模型修改|
|RAG|上下文优化|模型缺乏特定/实时知识|中，外挂知识库，无模型修改|
|微调（PEFT）|LLM优化|需改变模型行为/技能（如格式遵循、风格模仿、领域行话适配）|高，直接修改模型参数|
