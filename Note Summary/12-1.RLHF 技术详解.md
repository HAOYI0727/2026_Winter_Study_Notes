# RLHF 技术详解

<p align="right">2026.02.20</p>

## 1. 核心背景

### 1.1. SFT（有监督微调）的不足

- 泛化能力弱：仅能响应训练数据中的指令模式，无法覆盖多样真实意图
- 对齐不足：回答仅保证事实正确，在风格、语气、安全性、有用性上不符合人类偏好

### 1.2. RLHF 核心价值

**基于人类反馈的强化学习**（Reinforcement Learning from Human Feedback, **RLHF**），核心目标是让模型理解并内化人类价值观，输出符合“**有用、无害、诚实**（3H）”原则。实验验证：13亿参数的 RLHF 对齐模型（InstructGPT）人类评估表现超1750亿参数的原始 GPT-3。

### 1.3. SFT 与 RLHF 核心差异
|对比维度|监督微调 (SFT)|人类反馈强化学习 (RLHF)|
|---|---|---|
|核心目标|模仿正确答案（指令遵循）|对齐人类偏好（3H 原则）|
|数据需求|高质量 (指令, 回答) 对|提示词 + 偏好排序/评分|
|学习方式|填鸭式（拟合分布）|探索式（试错与反馈）|
|优化信号|Token 级预测概率（交叉熵）|整句生成质量评分（标量奖励）|
|泛化能力|局限于训练数据分布|可泛化到未见过的复杂指令|

## 2. 对齐模型

### 2.1. 基础模型预训练
- **数据**：万亿级无标注文本（网页、书籍、代码等）
- **任务**：自回归预测下一个词，习得基础语言生成能力
- **特点**：算力成本高，需解决大规模模型收敛问题

### 2.2. 有监督指令微调（SFT）
- **数据**：千到几万条高质量“**指令-回答**”对，分两类：
    - 任务型（如 WizardLM、Dolly）：教会模型“做事”，完成特定任务
    - 对话型（如 OpenAssistant）：教会模型“聊天”，多轮对话+真实反馈
- **技术**：可结合 **QLoRA** 等参数高效微调
- **目标**：让模型理解并执行指令，得到“**指令模型**”

### 2.3. 基于人类反馈的强化学习（RLHF）
- 核心：引入人类价值观作为指导信号，通过“**试错**”学习更优回复
- 目标：突破模仿学习局限，实现偏好对齐
- 形式化定义（MDP 框架）：

|要素|定义|
|---|---|
|状态 ( $s_t$ )|当前上下文 = 用户提示  $x$  + 已生成 Token 序列  $y_{<t}$ |
|动作 ( $a_t$ )|模型生成的下一个 Token  $y_t$ |
|策略 ( $\pi_\theta$ )|大语言模型，$\pi(a_t\|s_t)$ 对应模型在当前上下文下预测下一个 Token 的概率分布|
|奖励 ( $R$ )|回答生成完毕后，奖励模型给出的标量分数  $R(x, y)$ （中间步骤奖励为0）|

## 3. RLHF 核心步骤

### 3.1. 训练奖励模型（RM）：打造“人类偏好裁判”

（1）**收集偏好数据**
- 步骤：给指令 → SFT 模型生成多版本回答 → 人工排序（最好→最差）
- 数据格式： $\mathcal{D} = {(x, y_w, y_l)}$ （ $y_w$  更优， $y_l$  较差）

（2）**训练奖励模型**
- 数据转换：K 个回答的排序 →  $\binom{K}{2}$  个成对比较样本
- 模型结构：与语言模型类似，输入 (prompt, response) 输出标量奖励
- 损失函数（基于 Bradley-Terry 模型）：
    $\text{loss}(\theta) = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log(\sigma(r_\theta(x, y_w) - r_\theta(x, y_l))) \right]$ 

    目标：最大化偏好回答与非偏好回答的分数差距

### 3.2. 策略优化算法：微调模型最大化奖励

|算法|核心思路|适用场景|
|---|---|---|
|**PPO**|**近端策略优化**，通过裁剪代理目标限制策略更新幅度，避免训练崩溃；PPO-ptx 加入**预训练梯度**缓解“对齐税”|通用任务、需细粒度控制模型行为（如多轮对话），计算成本高|
|**DPO**|**直接用偏好数据优化策略**，绕过显式奖励模型和在线采样，损失函数简洁|样本稀缺、追求训练效率（如对话风格微调），**无需复杂 RL 框架**|
|**GRPO**|**去 Critic 化**，通过**组采样+组内优势**估计优化策略，KL 正则化降低显存占用|奖励在序列末尾的推理任务（如数学、代码），需在线探索涌现 CoT 能力|

> 对齐税：PPO 优化导致模型在传统 NLP 任务（如 SQuAD）性能下降；解决——PPO-ptx 混合预训练数据梯度，保留通用语言能力

### 3.3. RLHF 实际效果（以 InstructGPT 为例）

- 提升真实性：TruthfulQA 上真实且信息丰富的回答翻倍，捏造事实比例减半
- 降低有害性：有毒输出比例减少约25%（遵“尊重”指令时）
- 未解决问题：社会偏见无改善，仍会盲从错误指令、含糊其辞

## 4. RLHF 实践挑战与前沿方向

### 4.1. 核心挑战
- 奖励过拟合：模型利用奖励模型漏洞生成“高分无意义”内容
- 评估困境：难以全面评估模型是否真对齐人类价值观
- 多模态对齐：扩展到视频/音频时需解决视觉幻觉、时序理解问题
- 文化价值观对齐：偏好数据单一，易放大偏见
- 效率成本：PPO 等方法计算成本高，需轻量化方案

### 4.2. 前沿方向
- RLAIF：用 AI 生成偏好信号，替代/补充人类标注，降低成本
- 迭代式后训练：循环“采样→标注→训练”，实现模型自我进化，如 LLaMA 3）
- 推理模型：结合 RLVR 和 CoT，让模型涌现复杂逻辑推理能力（如 OpenAI o1、DeepSeek-R1）