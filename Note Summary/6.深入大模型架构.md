# 第六章 深入大模型架构

<p align="right">2026.02.10</p>

# 1. 手搓Llama2大模型

## 1.1. 核心架构
Llama2 采用 **Decoder-Only** 架构（GPT 系列一脉相承），核心是堆叠 N 个相同的 Transformer Block，整体数据流：
1. **输入嵌入**：token_ids → 词向量（[batch_size, seq_len, dim]）
2. **N×Transformer Block 堆叠**：预归一化 + 注意力子系统 + 前馈网络子系统 + 残差连接
3. **最终输出**：最后一次 RMSNorm + 线性层 → 词汇表 logits

| 核心改进点 | 对比传统 Transformer |
|---|---|
| **归一化** | **预归一化**（Pre-Norm）+ **RMSNorm**（简化LayerNorm，移除均值中心化） |
| **位置编码** | **旋转位置编码**（**RoPE**，相对位置编码，通过复数乘法旋转Q/K） |
| **注意力** | **分组查询注意力**（**GQA**，多Q头共享KV头，降低显存/计算开销） |
| **前馈网络** | **SwiGLU 激活**（**门控机制**，提升非线性表达） |

## 1.2. 关键组件

### 1.2.1. 预归一化：RMSNorm

（1）核心思路：简化 LayerNorm，仅保留“**均方根缩放 + 可学习增益**”，提升计算效率且保证训练稳定性。

（2）公式：$$y = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2 + \epsilon}} \cdot \gamma$$
（$\gamma$ 为可学习权重，$\epsilon$ 防止除零）

（3）代码实现：

```python
class RMSNorm(nn.Module):
    '''
    - 输入：[batch_size, seq_len, dim]
    - 输出：同输入形状，每个词元的dim维度独立归一化
    '''
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim)) # 对应公式中的 gamma

    def _norm(self, x: torch.Tensor) -> torch.Tensor:
        # 核心计算：x * (x^2的均值 + eps)的平方根的倒数
        return x * torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        out = self._norm(x.float()).type_as(x)
        return out * self.weight
```

### 1.2.2. 旋转位置编码（RoPE）
（1）核心思路：**相对位置编码：位置信息通过“复数乘法旋转Q/K向量”注入，而非加法**；优势：平移不变性（仅关注相对距离）、长度外推能力、计算高效。

（2）关键函数
| 函数 | 功能 | 核心逻辑 |
|---|---|---|
| precompute_freqs_cis | **预计算旋转角度复数张量** | 生成模为1的复数 $e^{im\theta}$（m为位置），形状[end, dim/2] |
| reshape_for_broadcast | **辅助广播** | 调整freqs_cis形状，适配Q/K的维度广播 |
| apply_rotary_emb | **应用RoPE到Q/K** | 将Q/Kreshape为复数形式，与freqs_cis相乘（旋转）后还原为实数 |

（3）代码实现
```Python
def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:
    # 1. 计算频率：1 / (theta^(2i/dim))
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # 2. 生成位置序列 t = [0, 1, ..., end-1]
    t = torch.arange(end, device=freqs.device)
    # 3. 计算相位：t 和 freqs 的外积
    freqs = torch.outer(t, freqs).float()
    # 4. 转换为复数形式 (cos(theta) + i*sin(theta))
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis

def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    ndim = x.ndim
    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)

def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor,) -> tuple[torch.Tensor, torch.Tensor]:
    # 将 Q/K 向量视为复数
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    
    # 准备广播
    freqs_q = reshape_for_broadcast(freqs_cis, xq_)  # 针对 Q 的广播视图
    # 复数乘法即为旋转
    xq_out = torch.view_as_real(xq_ * freqs_q).flatten(3)
    # K 向量可能与 Q 向量有不同的头数（GQA），所以需单独生成广播视图
    freqs_k = reshape_for_broadcast(freqs_cis, xk_)
    xk_out = torch.view_as_real(xk_ * freqs_k).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xq)
```

### 1.2.3. 分组查询注意力（GQA）

（1）核心思路：
- **MHA**：1个Q头对应1个KV头（n_heads = n_kv_heads）；
- **GQA：多Q头共享1个KV头**（n_heads > n_kv_heads），降低KV缓存/计算量；
- 关键：通过**repeat_kv**复制KV头，使数量与Q头匹配。

（2）关键函数/类
| 组件 | 功能 | 核心逻辑 |
|---|---|---|
| repeat_kv | **复制KV头** | 扩展+reshape，将KV从[n_kv_heads]→[n_heads] |
| GroupedQueryAttention | GQA实现 | **1. Q/K/V投影（WQ维度≠WK/WV）；2. 应用RoPE；3. 复制KV头；4. 计算注意力** |

（3）代码实现
```python
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    batch_size, seq_len, n_kv_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (x[:, :, :, None, :]
        .expand(batch_size, seq_len, n_kv_heads, n_rep, head_dim)
        .reshape(batch_size, seq_len, n_kv_heads * n_rep, head_dim)
    )

class GroupedQueryAttention(nn.Module):
    def __init__(self, dim: int, n_heads: int, n_kv_heads: int | None = None, ...):
        ...
        self.n_local_heads = n_heads
        self.n_local_kv_heads = n_kv_heads
        self.n_rep = self.n_local_heads // self.n_local_kv_heads # Q头与KV头的重复比
        ...
        self.wq = nn.Linear(dim, n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(dim, n_kv_heads * self.head_dim, bias=False)
        ...

    def forward(self, x, start_pos, freqs_cis, mask):
        xq = self.wq(x).view(batch_size, seq_len, self.n_local_heads, self.head_dim)
        xk = self.wk(x).view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = self.wv(x).view(batch_size, seq_len, self.n_local_kv_heads, self.head_dim)

        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
        
        # ... KV Cache 逻辑 ...

        keys = repeat_kv(keys, self.n_rep)   # <-- 关键步骤
        values = repeat_kv(values, self.n_rep) # <-- 关键步骤

        scores = torch.matmul(xq.transpose(1, 2), keys.transpose(1, 2).transpose(2, 3)) / ...
        ...
```

### 1.2.4. SwiGLU 前馈网络

（1）核心思路：**门控机制**提升非线性表达，公式：
$$\text{SwiGLU}(x, W, V, W_2) = (\text{swish}(xW) \otimes xV)W_2$$
（$\otimes$ 逐元素乘法，swish = x·sigmoid(x)）

（2）代码实现
```python
class FeedForward(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, ...):
        super().__init__()
        # hidden_dim 计算，并用 multiple_of 对齐以提高硬件效率
        hidden_dim = int(2 * hidden_dim / 3)
        ...
        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)

        self.w1 = nn.Linear(dim, hidden_dim, bias=False) # 对应 W
        self.w2 = nn.Linear(hidden_dim, dim, bias=False) # 对应 W2
        self.w3 = nn.Linear(dim, hidden_dim, bias=False) # 对应 V

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # F.silu(self.w1(x)) 实现了 swish(xW)
        # * self.w3(x) 实现了门控机制
        return self.w2(torch.nn.functional.silu(self.w1(x)) * self.w3(x))
```

## 1.3. 模型组装
### 1.3.1. TransformerBlock（基本单元）

```python
class TransformerBlock(nn.Module):
    def __init__(self, layer_id: int, ...):
        ...
        self.attention = GroupedQueryAttention(...)
        self.feed_forward = FeedForward(...)
        self.attention_norm = RMSNorm(...)
        self.ffn_norm = RMSNorm(...)

    def forward(self, x, start_pos, freqs_cis, mask):
        # 预归一化 + 残差连接
        h = x + self.attention(self.attention_norm(x), start_pos, freqs_cis, mask)
        out = h + self.feed_forward(self.ffn_norm(h))
        return out
```

### 1.3.2. LlamaTransformer（顶层模型）

（1）核心结构：
1. **tok_embeddings**：token→词向量；
2. **layers**：堆叠N个TransformerBlock；
3. **norm**：最终RMSNorm；
4. **output**：线性层→logits；
5. **freqs_cis**：预计算RoPE旋转矩阵（注册为buffer）。

（2）前向流程
```python
class LlamaTransformer(nn.Module):
    def __init__(self, vocab_size: int, ...):
        ...
        self.tok_embeddings = nn.Embedding(vocab_size, dim)
        self.layers = nn.ModuleList([TransformerBlock(...) for i in range(n_layers)])
        self.norm = RMSNorm(dim, eps=norm_eps)
        self.output = nn.Linear(dim, vocab_size, bias=False)
        self.register_buffer("freqs_cis", precompute_freqs_cis(...))

    def forward(self, tokens: torch.Tensor, start_pos: int) -> torch.Tensor:
        h = self.tok_embeddings(tokens)
        
        # 1. 准备 RoPE 旋转矩阵
        freqs_cis = self.freqs_cis[start_pos : start_pos + seq_len]
        # 2. 准备因果掩码 (Causal Mask)
        mask = None
        if seq_len > 1:
            mask = torch.full((seq_len, seq_len), float("-inf"), device=tokens.device)
            mask = torch.triu(mask, diagonal=1)
            # 考虑 KV Cache 的偏移
            mask = torch.hstack([torch.zeros((seq_len, start_pos), ...), mask]).type_as(h)
        # 3. 循环通过所有 TransformerBlock
        for layer in self.layers:
            h = layer(h, start_pos, freqs_cis, mask)
        
        h = self.norm(h)
        logits = self.output(h).float()
        return logits
```

# 2. MoE架构

## 2.1. 核心背景

### 2.1.1. 产生动机
稠密模型（如Llama2、GPT-3）所有参数参与每个Token计算，参数规模迈向万亿级时算力成本不可承受；MoE通过**稀疏激活机制**，兼顾大规模参数的知识容量与低推理成本，成为开源大模型核心技术方向。

### 2.1.2. 核心对比
| 对比维度 | MoE | 稠密模型 | 集成学习 |
|---|---|---|---|
| 计算方式 | **数据驱动动态激活部分子网络** | 全参数参与计算 | 所有基模型均参与计算 |
| 核心逻辑 | **条件计算、分治** | 全局统一计算 | 投票/加权平均 |

## 2.2. 技术演进

### 2.2.1. 早期探索

（1）1991（**自适应局部专家混合**）：**解决多任务学习“干扰效应”**
- 分治架构：**专家网络**（专注局部任务）+ **门控网络**（分配权重）；
- **竞争型损失函数**（负对数似然）实现“赢家通吃”，仅更新胜出专家权重。

（2）2013（**深度MoE**）：**从浅层到深层**
- **MoE模块化嵌入深度网络**，层级化门控实现指数级组合路径；
- 自动特征解耦（如MNIST任务中分层学习“位置/类别”特征）。

### 2.2.2. 规模化突破

（1）2017（**稀疏门控MoE**）：
- 条件计算：**激活少量专家实现超大参数量；解决稀疏性+负载均衡**（噪声门控+辅助损失防专家崩塌）；
- 1370亿参数，算力小幅损失下性能超SOTA

（2）2020（**GShard**）：
- **Transformer适配**：隔层替换FFN为MoE层，Top-2门控成标准；
- **分布式并行**：Attention复制、MoE分片，All-to-All通信；
- 6000亿参数，参数量增16倍算力仅增4倍

（3）2021（**Switch Transformer**）
- Top-1路由（单专家）简化计算/降低通信成本；
- **负载均衡**：**专家容量+Token丢弃+辅助损失**；
- **训练优化**：z-loss、选择性精度、小方差初始化；
- 1.6万亿参数，训练速度超T5-XXL 4倍

（4）2021（**GLaM**）：
- **Decoder-only架构适配MoE**，隔层替换+Top-2路由；高质量数据过滤提升性能；
- 1.2万亿参数，推理算力仅GPT-3的50%，训练能耗降64.6% 

### 2.2.3. 开源实践
（1）Mistral 8x7B：
- 470亿总参数，仅激活130亿（Top-2路由，8个专家/层）；
- **专家分工：基于语法/Token结构，非知识领域**；
- 13B活跃参数超越Llama2 70B/GPT-3.5，支持32k上下文

（2）DeepSeekMoE/R1：
- **细粒度专家分割（拆分专家+倍增激活数）**；
- **共享专家隔离（通用知识+专用知识分离）**；
- 数学/代码竞赛性能比肩OpenAI o1，Codeforces Elo达2029

## 2.3. MoE核心技术要点
### 2.3.1. 核心组件
（1）**专家网络**：独立子网络，专注局部任务/特征，通常为**双层全连接（FFN）**

（2）**门控网络**：输入Token计算专家权重，**动态选择Top-k专家**，核心是路由策略（Top-1/Top-2）

（3）**负载均衡**：**辅助损失（Importance/Load Loss）、专家容量、Token丢弃、共享专家**等，防止“专家饿死/赢家通吃”

### 2.3.2. Transformer适配原则
（1）**保留Attention层**（参数量小、计算关键），隔层替换FFN为MoE层；

（2）**分布式训练**：Attention层复制，MoE层分片存储，All-to-All通信。

### 2.3.3. 关键损失函数
（1）**竞争型损失**：$Loss_{comp} = - \log \sum_{i} p_i e^{-\frac{1}{2} || \mathbf{y} - \mathbf{E}_i ||^2}$，实现赢家通吃

（2）**Switch辅助损失**：$Loss_{aux} = \alpha \cdot N \cdot \sum_{i=1}^{N} f_i \cdot P_i$，强制专家负载均衡

## 2.4. MoE实战（Llama2改造）

### 2.4.1. 改造思路
**仅替换Transformer Block中的FFN层为MoE层，核心组件（Attention/RoPE/Norm）不变。**

### 2.4.2. 核心代码逻辑（MoE层）
```python
class MoE(nn.Module):
    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, ffn_dim_multiplier: Optional[float], num_experts: int = 8, top_k: int = 2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        # 门控网络：决定每个 Token 去往哪个专家
        self.gate = nn.Linear(dim, num_experts, bias=False)
        # 专家列表：创建 num_experts 个独立的 FeedForward 网络
        self.experts = nn.ModuleList([
            FeedForward(dim, hidden_dim, multiple_of，ffn_dim_multiplier)
            for _ in range(num_experts)
        ])

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch_size, seq_len, dim)
        B, T, D = x.shape
        x_flat = x.view(-1, D)
        
        # 1. 门控网络
        gate_logits = self.gate(x_flat) # (B*T, num_experts)
        # 2. Top-k 路由
        weights, indices = torch.topk(gate_logits, self.top_k, dim=-1)
        weights = F.softmax(weights, dim=-1) # 归一化权重
        output = torch.zeros_like(x_flat)
        
        for i, expert in enumerate(self.experts):
            # 3. 找出所有选中当前专家 i 的 token 索引
            batch_idx, k_idx = torch.where(indices == i)
            if len(batch_idx) == 0:
                continue
                
            # 4. 取出对应的输入进行计算
            expert_input = x_flat[batch_idx]
            expert_out = expert(expert_input)
            # 5. 获取对应的权重
            expert_weights = weights[batch_idx, k_idx].unsqueeze(-1) # (num_selected, 1)
            # 6. 将结果加权累加回输出张量
            output.index_add_(0, batch_idx, expert_out * expert_weights)
            
        return output.view(B, T, D)
```

### 2.4.3. 关键注意点
（1）**显存占用**：虽仅激活部分参数，但需加载所有专家参数（如Mistral 8x7B显存仍为47B级别）；

（2）**推理效率**：激活参数少，推理速度接近小模型（如GLaM推理算力仅GPT-3的50%）。

# 3. 大模型生成策略

## 3.1. 逐Token生成核心流程
### 3.1.1. Pipeline接口的完整调用链
| 步骤 | 核心操作 | 关键细节 |
|---|---|---|
| **预处理（Preprocess）** | **文本转模型可识别张量** | **tokenizer**将prompt转为`input_ids`（Token ID张量）和`attention_mask`；保留原始`prompt_text`用于后处理拼接 |
| **入口分发** | **调用`model.generate()`** | **Pipeline**仅整理参数，真正解码在`model.generate()`；合并用户参数与`GenerationConfig`默认配置 |
| **策略选择** | **确定解码模式** | **根据配置自动选择**：Greedy/Sampling/Beam Search等，对应不同实现方法（如`_sample()`/`_greedy_search()`） |
| **解码循环（核心）** | **逐Token生成** | 1. **准备输入**：`prepare_inputs_for_generation()`处理KV Cache和当前输入<br>2. **模型前向**：得到最后一个位置的logits<br>3. **规则修正**：`LogitsProcessor`修改概率分布（温度、TopK/TopP等）<br>4. **采样/贪心**：选Next Token（`multinomial`采样/`argmax`贪心）<br>5. **拼接更新**：新Token拼到`input_ids`末尾<br>6. **停止判定**：`StoppingCriteria`判断是否终止（EOS/最大长度） |
| **后处理（Postprocess）** | **张量转回文本** | `tokenizer.decode()`还原Token ID为字符串；根据配置返回FULL_TEXT（含prompt）/NEW_TEXT（仅新增内容） |

### 3.1.2. 关键概念解析
（1）**KV Cache**：**缓存每一层注意力的key/value**，复用历史计算结果，加速逐Token生成，随生成过程逐步增长。

（2）**GenerationConfig核心参数**：
  | 参数 | 作用 |
  |------|------|
  | `bos_token_id` | 生成起始Token（无输入时起头） |
  | `eos_token_id` | 生成结束Token（遇到则停止） |
  | `do_sample` | **是否采样**（True：随机多样；False：确定） |
  | `temperature` | **采样温度（越小越保守，越大越随机）** |
  | `max_new_tokens` | 新增Token上限 |
  | `max_length` | 总序列长度上限（prompt+新生成） |

## 3.2. Logits规则链与解码策略

### 3.2.1. Logits规则链（生成每一步的概率修正）

（1）**构造逻辑**：`_get_logits_processor()`将生成控制参数转换为`LogitsProcessorList`（规则链），按顺序执行。

（2）**常见修正规则（Warper）**：
  | 规则 | 作用 |
  |---|---|
  | `TemperatureLogitsWarper` | **温度缩放（logits/T），调整概率分布尖锐度** |
  | `TopKLogitsWarper` | **保留Top-K高分Token，其余置为`-inf`（候选数固定）** |
  | `TopPLogitsWarper` | **保留累计概率≥P的最小Token集（候选数动态）** |
  | `TypicalP/Epsilon/EtaCutoff` | **按典型性/概率阈值过滤Token** |

（3）**执行逻辑**：**每一步生成时，logits先经过规则链修正，再通过softmax转为概率分布，最终用于选Next Token。**

### 3.2.2. 常用解码策略
| 策略 | 触发条件 | 核心逻辑 | 优缺点 | 适用场景 |
|---|---|---|---|---|
| **Greedy Search（贪心）** | `num_beams=1`+`do_sample=False` | **每步选logits最大的Token（`argmax`）** | 优点：速度快、稳定；缺点：易局部最优、重复/套路化 | 基线测试、强确定性场景 |
| **Sampling（采样）** | `do_sample=True` | **修正后概率分布上随机采样（`multinomial`）** | 优点：输出多样；缺点：可控性稍弱 | 开放式生成（对话/续写） |
| **Beam Sample（束采样）** | `num_beams>1`+`do_sample=True` | **Beam框架+采样，兼顾搜索能力与多样性** | 优点：折中稳定与多样；缺点：计算稍复杂 | 需平衡多样性与质量的场景 |
| **Beam Search（束搜索）** | `num_beams>1`+`do_sample=False` | **维护N条候选序列，按累计分数剪枝，选序列级最优** | 优点：结果稳定、序列最优；缺点：保守、易重复、计算开销大 | 翻译/摘要等非开放式生成 |

### 3.2.3. 策略调优要点

（1）`temperature`+`top_p`：**温度控随机性**，Top-P裁低概率尾巴，是开放式生成常用组合。

（2）`top_k`+`top_p`：**双重过滤**，候选为两者交集，进一步限制采样范围。

## 3.3. 调试技巧

（1）**断点核心位置**：
   - **Pipeline层**：`preprocess()`返回、`_forward()`的`model.generate()`、`postprocess()`的decode/返回逻辑。
   - **Generate层**：`_prepare_generation_config`、`decoding_method`调用、`return result`。

（2）**调试思路**：
   - 从陌生代码的核心调用行（如`generator(...)`）断点，逐步步入。
   - 关注变量窗口，通过`Ctrl+B`（转到定义）回溯关键变量来源。
   - 先定位“输入-输出”关键节点，再反向回溯细节逻辑，按需补充断点。

（3）**注意事项**：调试器停在断点“执行前”，需单步执行才能看到变量最终值。