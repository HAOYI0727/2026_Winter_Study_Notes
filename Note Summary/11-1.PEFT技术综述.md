# PEFT技术综述

<p align="right">2026.02.11</p>

## 1. 背景：大模型微调的困境

传统全量微调千亿级大模型面临核心问题：
1. **成本高昂**：需数百GB显存，时间/算力成本极高；
2. **存储压力**：每个任务保存完整模型副本，存储开销不可承受；
3. **灾难性遗忘**：微调特定任务时丢失预训练的通用知识；
4. **训练不稳定**：网络结构复杂，易出现梯度消失/爆炸。

过渡：从提示范式到PEFT
1. **硬提示（In-Context Learning）**：人工设计离散文本提示，无参数更新。局限是试错成本高、表达能力有限、跨模型/语言效果差。
2. **参数高效微调（Parameter-Efficient Fine-Tuning，PEFT）**：冻结99%+预训练模型参数，仅微调<1%参数或新增小参数。低成本适配下游任务，借鉴CV迁移学习思想。

## 2. 主流PEFT技术路线

### 2.1. Adapter Tuning

（1）**核心思路**：在Transformer每个块的注意力层/前馈网络内插入**小型Adapter模块**，仅更新Adapter参数；

（2）**模块结构**：**降维**全连接层→非线性激活→**升维**全连接层+残差连接（瓶颈式结构）；

（3）**优缺点**：参数效率高、训练稳定、性能接近全量微调；但增加激活内存/算力开销，千亿级模型工程实现难。

### 2.2. Prefix Tuning

（1）**核心思路**：模型外部添加**可训练的前缀激活值**（Prefix Activations），原模型参数冻结；

（2）**实现细节**：输入层+Transformer每一层都添加可学习Prefix，由**小型MLP**生成；

（3）**优缺点**：参数效率极高、显存友好、通用性强；训练不稳定（对超参数/初始化敏感）、占用注意力上下文窗口。

### 2.3. Prompt Tuning

（1）**核心思路**：Prefix Tuning简化版，仅在Embedding层添加可学习的虚拟Token（Soft Prompt）；

（2）**核心优势**：极致轻量化：仅训练万级参数的Soft Prompt，大模型完全冻结共享；支持混合任务批处理，训练吞吐高；模型规模缩放效应：模型>100亿参数时，性能追平/超越全量微调；

（3）**局限**：强依赖超大模型规模，中小模型效果差。

## 3. P-Tuning v2：通用型PEFT方案

### 3.1. 前身P-Tuning v1（解决离散提示问题）

（1）**核心改进**：从离散文本空间转向**连续向量空间优化提示**；

（2）**实现**：引入**Prompt Encoder**（如LSTM），训练可学习的**伪提示**（Pseudo Prompts），生成最优提示向量；

（3）**局限**：对模型规模敏感、复杂NLU任务（序列标注）表现差。

### 3.2. P-Tuning v2核心演进

|改进方向|具体做法|效果|
|---|---|---|
|**深层提示**|提示向量注入Transformer每一层（借鉴Prefix Tuning）|增强对模型的控制力，中小模型也有效|
|**移除Verbalizer**|回归传统线性分类头，不再强行包装成完形填空任务|适配复杂任务（序列标注、抽取式阅读理解）|

**本质**：融合Prefix Tuning多层结构 + 传统微调输出头 + Prompt Tuning轻量化，仅微调0.1%~3%参数，兼顾效率与通用性。

> 总结
> |技术|核心特征|适用场景|
> |---|---|---|
> |Adapter Tuning|模型内部插模块，参数适中|对算力有一定容忍，追求稳定性能|
> |Prefix Tuning|多层外部前缀，显存友好|显存受限，需适配多类型模型（GPT/T5）|
> |Prompt Tuning|仅输入层软提示，极致轻量化|超大模型（>100亿）、多任务共享场景|
> |P-Tuning v2|深层提示+通用输出头|全规模模型、全类型任务（推荐首选）|
> 
> PEFT技术核心目标：以极低的参数/算力成本，让大模型适配下游任务，兼顾通用性、稳定性与效率，其中P-Tuning v2是当前覆盖场景最广的通用方案。
